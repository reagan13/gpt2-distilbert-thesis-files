{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.46.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch scikit-learn pandas tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"augmented_data1_final.csv\")  # Ensure CSV has 'text' and 'intent' columns\n",
    "\n",
    "# Encode labels\n",
    "df['intent'] = df['intent'].astype('category').cat.codes  # Convert intent to numerical labels\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['instruction_augmented'].tolist(), df['intent'].tolist(), test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easynmt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for fasttext (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [31 lines of output]\n",
      "      C:\\Users\\reaga\\AppData\\Local\\Temp\\pip-build-env-nvoz3vvu\\overlay\\Lib\\site-packages\\setuptools\\dist.py:493: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2025-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-cpython-311\\fasttext\n",
      "      copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\n",
      "      creating build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "      copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "      creating build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "      running build_ext\n",
      "      building 'fasttext_pybind' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (fasttext)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached EasyNMT-2.0.2-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (4.67.1)\n",
      "Requirement already satisfied: transformers<5,>=4.4 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (4.46.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (1.26.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (0.2.0)\n",
      "Collecting fasttext (from easynmt)\n",
      "  Using cached fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: protobuf in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from easynmt) (5.29.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->easynmt) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->easynmt) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->easynmt) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->easynmt) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->easynmt) (2023.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.6.0->easynmt) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=1.6.0->easynmt) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5,>=4.4->easynmt) (0.4.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->easynmt) (0.4.6)\n",
      "Collecting pybind11>=2.2 (from fasttext->easynmt)\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from fasttext->easynmt) (65.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->easynmt) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk->easynmt) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.6.0->easynmt) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers<5,>=4.4->easynmt) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers<5,>=4.4->easynmt) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers<5,>=4.4->easynmt) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reaga\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers<5,>=4.4->easynmt) (2023.5.7)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml): started\n",
      "  Building wheel for fasttext (pyproject.toml): finished with status 'error'\n",
      "Failed to build fasttext\n"
     ]
    }
   ],
   "source": [
    "!pip install easynmt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentaiton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "print(\"EasyNMT imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully!\n",
      "\n",
      "--- Before Dropping ---\n",
      "Total rows: 26872\n",
      "Missing values in 'instruction': 0\n",
      "\n",
      "--- After Dropping ---\n",
      "Total rows: 26872\n",
      "Missing values in 'instruction': 0\n",
      "\n",
      "Validation: No missing values in 'instruction' after dropping.\n",
      "   flags                                        instruction category  \\\n",
      "0      B   question about cancelling order {{Order Number}}    ORDER   \n",
      "1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n",
      "2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n",
      "3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n",
      "4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n",
      "\n",
      "         intent                                           response  \n",
      "0  cancel_order  I've understood you have a question regarding ...  \n",
      "1  cancel_order  I've been informed that you have a question ab...  \n",
      "2  cancel_order  I can sense that you're seeking assistance wit...  \n",
      "3  cancel_order  I understood that you need assistance with can...  \n",
      "4  cancel_order  I'm sensitive to the fact that you're facing f...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from CSV\n",
    "def load_dataset(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if 'instruction' not in df.columns or 'intent' not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain 'instruction' and 'intent' columns\")\n",
    "    return df\n",
    "\n",
    "csv_file = \"dataset.csv\"\n",
    "print(\"Loading dataset...\")\n",
    "df = load_dataset(csv_file)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# --- Analysis before dropping ---\n",
    "print(\"\\n--- Before Dropping ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing values in 'instruction': {df['instruction'].isnull().sum()}\")\n",
    "\n",
    "# --- Drop missing values ---\n",
    "df.dropna(subset=['instruction'], inplace=True)\n",
    "\n",
    "# --- Analysis after dropping ---\n",
    "print(\"\\n--- After Dropping ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing values in 'instruction': {df['instruction'].isnull().sum()}\")\n",
    "\n",
    "# --- Validation ---\n",
    "if df['instruction'].isnull().sum() == 0:\n",
    "    print(\"\\nValidation: No missing values in 'instruction' after dropping.\")\n",
    "else:\n",
    "    print(\"\\nValidation: There are still missing values in 'instruction'.\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique intents: 27\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2ForSequenceClassification\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load GPT-2 model for sequence classification (instead of just the base model)\n",
    "gpt2_model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=len(df['intent'].unique()))\n",
    "\n",
    "# Move the model to the GPU if available, otherwise, use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpt2_model.to(device)\n",
    "\n",
    "# Get the number of unique intents\n",
    "num_intents = len(df['intent'].unique())\n",
    "print(f\"Number of unique intents: {num_intents}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels...\n",
      "\n",
      "Intent Mapping:\n",
      "{'cancel_order': 0, 'change_order': 1, 'change_shipping_address': 2, 'check_cancellation_fee': 3, 'check_invoice': 4, 'check_payment_methods': 5, 'check_refund_policy': 6, 'complaint': 7, 'contact_customer_service': 8, 'contact_human_agent': 9, 'create_account': 10, 'delete_account': 11, 'delivery_options': 12, 'delivery_period': 13, 'edit_account': 14, 'get_invoice': 15, 'get_refund': 16, 'newsletter_subscription': 17, 'payment_issue': 18, 'place_order': 19, 'recover_password': 20, 'registration_problems': 21, 'review': 22, 'set_up_shipping_address': 23, 'switch_account': 24, 'track_order': 25, 'track_refund': 26}\n"
     ]
    }
   ],
   "source": [
    "# Encode labels for intent classification only\n",
    "def encode_labels(df):\n",
    "    # Create a mapping for intent labels\n",
    "    intent_map = {intent: idx for idx, intent in enumerate(df['intent'].unique())}\n",
    "\n",
    "    # Reverse mapping for decoding (optional)\n",
    "    reverse_intent_map = {v: k for k, v in intent_map.items()}\n",
    "\n",
    "    # Encode 'intent' column\n",
    "    df['intent_encoded'] = df['intent'].map(intent_map)\n",
    "\n",
    "    return df, intent_map, reverse_intent_map\n",
    "\n",
    "# Encode labels\n",
    "print(\"Encoding labels...\")\n",
    "df, intent_map, reverse_intent_map = encode_labels(df)\n",
    "\n",
    "# Print intent mappings\n",
    "print(\"\\nIntent Mapping:\")\n",
    "print(intent_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving preprocessed dataset as JSON...\n",
      "Preprocessed dataset saved to preprocessed_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed dataset as JSON\n",
    "def save_preprocessed_dataset(df, json_file):\n",
    "    # Convert DataFrame to JSON format\n",
    "    df.to_json(json_file, orient='records', lines=True)\n",
    "\n",
    "# Path to save the preprocessed JSON file\n",
    "json_file = \"preprocessed_dataset.json\"  # Replace with your desired output path\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "print(\"\\nSaving preprocessed dataset as JSON...\")\n",
    "save_preprocessed_dataset(df, json_file)\n",
    "print(f\"Preprocessed dataset saved to {json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset from JSON...\n",
      "\n",
      "Preprocessed Dataset:\n",
      "   flags                                        instruction category  \\\n",
      "0      B   question about cancelling order {{Order Number}}    ORDER   \n",
      "1    BQZ  i have a question about cancelling oorder {{Or...    ORDER   \n",
      "2   BLQZ    i need help cancelling puchase {{Order Number}}    ORDER   \n",
      "3     BL         I need to cancel purchase {{Order Number}}    ORDER   \n",
      "4  BCELN  I cannot afford this order, cancel purchase {{...    ORDER   \n",
      "\n",
      "         intent                                           response  \\\n",
      "0  cancel_order  I've understood you have a question regarding ...   \n",
      "1  cancel_order  I've been informed that you have a question ab...   \n",
      "2  cancel_order  I can sense that you're seeking assistance wit...   \n",
      "3  cancel_order  I understood that you need assistance with can...   \n",
      "4  cancel_order  I'm sensitive to the fact that you're facing f...   \n",
      "\n",
      "   intent_encoded  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "\n",
      "Encoded Intent Example:\n",
      "                                         instruction        intent  \\\n",
      "0   question about cancelling order {{Order Number}}  cancel_order   \n",
      "1  i have a question about cancelling oorder {{Or...  cancel_order   \n",
      "2    i need help cancelling puchase {{Order Number}}  cancel_order   \n",
      "3         I need to cancel purchase {{Order Number}}  cancel_order   \n",
      "4  I cannot afford this order, cancel purchase {{...  cancel_order   \n",
      "\n",
      "   intent_encoded  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset from JSON for intent classification only\n",
    "print(\"Loading preprocessed dataset from JSON...\")\n",
    "preprocessed_df = pd.read_json(json_file, lines=True)\n",
    "\n",
    "# Display the first few rows of the preprocessed dataset\n",
    "print(\"\\nPreprocessed Dataset:\")\n",
    "print(preprocessed_df.head())\n",
    "\n",
    "# Verify the encoded 'intent' column\n",
    "print(\"\\nEncoded Intent Example:\")\n",
    "print(preprocessed_df[['instruction', 'intent', 'intent_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token\n",
    "\n",
    "# Define a custom dataset class for intent classification\n",
    "class IntentClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['instruction']\n",
    "        intent_label = row['intent_encoded']\n",
    "\n",
    "        # Tokenize the input\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'intent_labels': intent_label\n",
    "        }\n",
    "\n",
    "# Create datasets (train/test split)\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = IntentClassificationDataset(train_data, tokenizer, max_length=128)\n",
    "val_dataset = IntentClassificationDataset(val_data, tokenizer, max_length=128)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "class IntentGPT2(nn.Module):\n",
    "    def __init__(self, gpt2_model, num_intents, dropout_prob=0.4):\n",
    "        super(IntentGPT2, self).__init__()\n",
    "        self.gpt2 = gpt2_model\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Dropout for intent only\n",
    "\n",
    "        self.intent_head = nn.Linear(768, num_intents)  # Classification head for intent\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through GPT-2 model\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Mean pooling for intent classification\n",
    "        cls_like_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "        cls_like_embedding = self.dropout(cls_like_embedding)  # Apply dropout\n",
    "\n",
    "        # Intent classification\n",
    "        intent_logits = self.intent_head(cls_like_embedding)\n",
    "\n",
    "        return intent_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss for intent classification to handle class imbalance.\n",
    "\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Class weights (inverse frequency) for handling imbalance.\n",
    "            gamma (int, optional): Focusing parameter to adjust difficulty weighting (default: 2).\n",
    "            reduction (str, optional): Specifies 'mean', 'sum', or 'none' reduction (default: 'mean').\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Class weights (optional, computed from inverse frequency)\n",
    "        self.gamma = gamma  # Focusing parameter to handle difficult examples\n",
    "        self.reduction = reduction  # Specifies how the loss should be reduced ('mean', 'sum', 'none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the focal loss for intent classification.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): Model predictions (logits), shape: (batch_size, num_classes).\n",
    "            targets (Tensor): Ground truth labels, shape: (batch_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Computed focal loss.\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities using softmax\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)  # Log probabilities\n",
    "        probs = torch.exp(log_probs)  # Convert log_probs to probabilities\n",
    "\n",
    "        # Gather log_probs corresponding to the target class\n",
    "        targets = targets.view(-1, 1)  # Ensure correct shape for targets\n",
    "        log_pt = log_probs.gather(dim=-1, index=targets).squeeze(-1)  # Log-prob for target class\n",
    "        pt = probs.gather(dim=-1, index=targets).squeeze(-1)  # Probability for target class\n",
    "\n",
    "        # Compute the focal loss modulating factor\n",
    "        focal_weight = (1 - pt) ** self.gamma  # The modulating factor (1 - pt) ^ gamma\n",
    "\n",
    "        # Compute focal loss\n",
    "        focal_loss = -focal_weight * log_pt  # Apply modulating factor to log_pt\n",
    "\n",
    "        # Apply class weights (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha_factor = self.alpha[targets.squeeze(-1)]  # Weights for target class\n",
    "            focal_loss = alpha_factor * focal_loss  # Apply class weighting\n",
    "\n",
    "        # Apply reduction (mean, sum, or none)\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss  # No reduction, return per sample loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEvice cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reaga\\AppData\\Local\\Temp\\ipykernel_5688\\1155686904.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Training:   0%|          | 0/1344 [00:00<?, ?it/s]C:\\Users\\reaga\\AppData\\Local\\Temp\\ipykernel_5688\\1155686904.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 128\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# TRAINING LOOP WITH VALIDATION LOSS\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintent_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 69\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(dataloader, model, optimizer, intent_loss_fn, device)\u001b[0m\n\u001b[0;32m     66\u001b[0m     intent_loss \u001b[38;5;241m=\u001b[39m intent_loss_fn(intent_logits, intent_labels)\n\u001b[0;32m     68\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(intent_loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 69\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     72\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m intent_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ============================\n",
    "# HYPERPARAMETERS & SETUP\n",
    "# ============================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"DEvice\", device)\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# ============================\n",
    "# COMPUTE CLASS WEIGHTS\n",
    "# ============================\n",
    "num_intents = len(df['intent_encoded'].unique())\n",
    "\n",
    "# Compute intent class weights\n",
    "intent_weights = compute_class_weight('balanced', classes=np.arange(num_intents), y=df['intent_encoded'])\n",
    "\n",
    "# Convert to tensor and move to GPU\n",
    "intent_weights = torch.tensor(intent_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# ============================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================\n",
    "intent_loss_fn = nn.CrossEntropyLoss(weight=intent_weights)\n",
    "\n",
    "# ============================\n",
    "# MODEL SETUP\n",
    "# ============================\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "model = IntentGPT2(gpt2_model, num_intents).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ============================\n",
    "# TRAINING FUNCTION\n",
    "# ============================\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_model(dataloader, model, optimizer, intent_loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        intent_labels = batch['intent_labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            intent_logits = model(input_ids, attention_mask)\n",
    "            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "\n",
    "        scaler.scale(intent_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += intent_loss.item()\n",
    "        progress_bar.set_postfix(loss=intent_loss.item())\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# EVALUATION FUNCTION WITH VALIDATION LOSS\n",
    "# ============================\n",
    "def evaluate_model(dataloader, model, intent_loss_fn, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            intent_labels = batch['intent_labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            intent_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(intent_logits, dim=1).cpu().numpy()\n",
    "            labels = intent_labels.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(num_intents)])\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(dataloader)  # Average validation loss\n",
    "    return accuracy, f1, avg_val_loss, conf_matrix, report\n",
    "\n",
    "# ============================\n",
    "# TRAINING LOOP WITH VALIDATION LOSS\n",
    "# ============================\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss = train_model(train_dataloader, model, optimizer, intent_loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    val_accuracy, val_f1, val_loss, val_conf_matrix, val_report = evaluate_model(val_dataloader, model, intent_loss_fn, device)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(val_conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(val_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilbert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully!\n",
      "\n",
      "--- Before Dropping ---\n",
      "Total rows: 26872\n",
      "Missing values in 'instruction_augmented': 111\n",
      "\n",
      "--- After Dropping ---\n",
      "Total rows: 26761\n",
      "Missing values in 'instruction_augmented': 0\n",
      "\n",
      "Validation: No missing values in 'instruction_augmented' after dropping.\n",
      "  category        intent                                           response  \\\n",
      "0    ORDER  cancel_order  I understand the financial constraints you are...   \n",
      "1    ORDER  cancel_order  I recognize that you've recently made a purcha...   \n",
      "2    ORDER  cancel_order  I've observed that you no longer want the item...   \n",
      "3    ORDER  cancel_order  I'm attuned to the idea that you require assis...   \n",
      "4    ORDER  cancel_order  I realized, you're looking for the information...   \n",
      "\n",
      "                                instruction_original  \\\n",
      "0          I cannot afford purchase {{Order Number}}   \n",
      "1  I have bought some item, I have to cancel orde...   \n",
      "2  I do not want this item, cancel order {{Order ...   \n",
      "3  I need assistance with canceling purchase {{Or...   \n",
      "4        were can i cancel purchase {{Order Number}}   \n",
      "\n",
      "                               instruction_augmented    augmentation_technique  \n",
      "0                                       Ich I mich i              Paraphrasing  \n",
      "1  Ich habe einen Artikel gekauft, ich habe das g...              Paraphrasing  \n",
      "2  I do not want this item, cancel order {{Order ...           None (Fallback)  \n",
      "3  I need assistance with canceling purchase uh {...  Noise (insertion, often)  \n",
      "4        were can i cancel purchase {{Order Number}}           None (Fallback)  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reaga\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\reaga\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique intents: 27\n",
      "Encoding labels...\n",
      "\n",
      "Intent Mapping:\n",
      "{'cancel_order': 0, 'change_order': 1, 'change_shipping_address': 2, 'check_cancellation_fee': 3, 'check_invoice': 4, 'check_payment_methods': 5, 'check_refund_policy': 6, 'complaint': 7, 'contact_customer_service': 8, 'contact_human_agent': 9, 'create_account': 10, 'delete_account': 11, 'delivery_options': 12, 'delivery_period': 13, 'edit_account': 14, 'get_invoice': 15, 'get_refund': 16, 'newsletter_subscription': 17, 'payment_issue': 18, 'place_order': 19, 'recover_password': 20, 'registration_problems': 21, 'review': 22, 'set_up_shipping_address': 23, 'switch_account': 24, 'track_order': 25, 'track_refund': 26}\n",
      "\n",
      "Saving preprocessed dataset as JSON...\n",
      "Preprocessed dataset saved to preprocessed_dataset.json\n",
      "Loading preprocessed dataset from JSON...\n",
      "\n",
      "Preprocessed Dataset:\n",
      "  category        intent                                           response  \\\n",
      "0    ORDER  cancel_order  I understand the financial constraints you are...   \n",
      "1    ORDER  cancel_order  I recognize that you've recently made a purcha...   \n",
      "2    ORDER  cancel_order  I've observed that you no longer want the item...   \n",
      "3    ORDER  cancel_order  I'm attuned to the idea that you require assis...   \n",
      "4    ORDER  cancel_order  I realized, you're looking for the information...   \n",
      "\n",
      "                                instruction_original  \\\n",
      "0          I cannot afford purchase {{Order Number}}   \n",
      "1  I have bought some item, I have to cancel orde...   \n",
      "2  I do not want this item, cancel order {{Order ...   \n",
      "3  I need assistance with canceling purchase {{Or...   \n",
      "4        were can i cancel purchase {{Order Number}}   \n",
      "\n",
      "                               instruction_augmented  \\\n",
      "0                                       Ich I mich i   \n",
      "1  Ich habe einen Artikel gekauft, ich habe das g...   \n",
      "2  I do not want this item, cancel order {{Order ...   \n",
      "3  I need assistance with canceling purchase uh {...   \n",
      "4        were can i cancel purchase {{Order Number}}   \n",
      "\n",
      "     augmentation_technique  intent_encoded  \n",
      "0              Paraphrasing               0  \n",
      "1              Paraphrasing               0  \n",
      "2           None (Fallback)               0  \n",
      "3  Noise (insertion, often)               0  \n",
      "4           None (Fallback)               0  \n",
      "\n",
      "Encoded Intent Example:\n",
      "                               instruction_augmented        intent  \\\n",
      "0                                       Ich I mich i  cancel_order   \n",
      "1  Ich habe einen Artikel gekauft, ich habe das g...  cancel_order   \n",
      "2  I do not want this item, cancel order {{Order ...  cancel_order   \n",
      "3  I need assistance with canceling purchase uh {...  cancel_order   \n",
      "4        were can i cancel purchase {{Order Number}}  cancel_order   \n",
      "\n",
      "   intent_encoded  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2710\n",
      "Validation Accuracy: 0.9238\n",
      "Validation F1 Score: 0.9244\n",
      "Confusion Matrix:\n",
      "[[180   1   0   0   0   1   0   0   0   0   1   0   0   1   0   0   1   0\n",
      "    1   1   1   0   0   0   0   4   0]\n",
      " [  8 165   0   0   0   0   0   0   0   0   0   0   1   0   0   1   0   0\n",
      "    0   2   1   0   0   0   1  12   0]\n",
      " [  2   2 197   0   0   0   0   2   1   0   1   1   1   1   0   1   0   0\n",
      "    0   0   1   0   0   9   0   0   0]\n",
      " [  7   0   0 185   0   3   0   0   0   0   0   0   3   2   0   0   0   0\n",
      "    0   1   0   0   1   0   0   1   0]\n",
      " [  3   1   0   0 171   0   0   2   1   1   1   1   0   3   0   3   0   1\n",
      "    0   0   1   0   0   1   1   0   0]\n",
      " [  0   0   0   0   0 197   0   0   0   1   1   0   2   0   0   0   0   1\n",
      "    5   0   2   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   2 171   2   1   1   0   0   2   4   0   0   2   0\n",
      "    0   1   1   0   0   0   1   0   2]\n",
      " [  3   0   0   0   0   2   1 178   1   0   0   0   0   0   0   0   0   0\n",
      "    1   2   1   0   2   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   1 199   1   0   0   0   2   0   0   0   0\n",
      "    0   0   0   1   4   1   0   0   0]\n",
      " [  3   0   0   0   1   0   0   2   1 200   0   0   0   0   0   0   0   0\n",
      "    1   0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   0   2   1   0   0   0   0 191   2   0   0   1   0   0   0\n",
      "    0   2   1   1   0   0   7   1   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   2 164   0   0   0   0   1   1\n",
      "    0   2   1   0   1   1   3   1   0]\n",
      " [  3   0   0   0   0   0   0   1   0   0   0   0 236   1   0   0   0   2\n",
      "    1   2   1   0   2   0   0   0   0]\n",
      " [  2   0   0   0   0   0   0   1   0   0   1   0   5 177   0   0   0   1\n",
      "    0   1   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   2   1   1   0   0 182   0   0   0\n",
      "    0   0   2   0   0   0   0   0   1]\n",
      " [  1   0   0   0  11   1   0   2   0   0   0   0   1   0   0 193   1   0\n",
      "    1   0   2   0   0   0   0   0   0]\n",
      " [  3   0   0   0   0   1   1   1   1   0   2   0   1   3   0   0 153   1\n",
      "    0   2   0   1   2   0   1   0   4]\n",
      " [  2   0   0   0   0   2   0   0   0   0   1   0   0   0   0   0   0 164\n",
      "    0   0   2   0   2   0   1   0   0]\n",
      " [  2   0   0   0   0   1   0   0   1   0   1   0   1   1   0   0   0   0\n",
      "  184   2   4   3   0   1   0   0   1]\n",
      " [  3   0   0   0   0   0   0   2   0   0   0   0   0   1   0   0   0   0\n",
      "    0 190   2   0   2   0   0   0   0]\n",
      " [  1   1   0   0   0   0   0   2   1   1   1   0   0   2   3   0   0   0\n",
      "    0   0 191   0   1   0   0   0   0]\n",
      " [  1   0   0   0   0   2   0   2   2   2   1   1   2   0   0   0   0   1\n",
      "    2   0   1 176   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   1   0   0   1   0   0   0   0   2\n",
      "    0   2   2   0 185   0   0   0   0]\n",
      " [  3   0   0   0   0   1   0   0   0   1   1   0   6   1   0   0   0   0\n",
      "    0   0   2   0   1 174   1   0   0]\n",
      " [  2   0   0   0   0   1   0   0   1   0   1   0   1   1   2   0   0   1\n",
      "    0   2   4   0   0   0 174   0   0]\n",
      " [  5   0   0   0   0   1   0   1   0   0   2   0   0   1   0   0   0   0\n",
      "    0   0   0   1   1   0   0 200   1]\n",
      " [  1   0   0   1   0   1   1   0   0   0   0   0   2   1   0   0   4   0\n",
      "    0   0   0   0   1   0   0   1 168]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.75      0.94      0.83       192\n",
      "     Class 1       0.96      0.86      0.91       191\n",
      "     Class 2       1.00      0.90      0.95       219\n",
      "     Class 3       0.99      0.91      0.95       203\n",
      "     Class 4       0.92      0.90      0.91       191\n",
      "     Class 5       0.91      0.94      0.92       209\n",
      "     Class 6       0.98      0.90      0.94       190\n",
      "     Class 7       0.89      0.93      0.91       191\n",
      "     Class 8       0.95      0.95      0.95       210\n",
      "     Class 9       0.95      0.96      0.95       208\n",
      "    Class 10       0.91      0.91      0.91       211\n",
      "    Class 11       0.96      0.92      0.94       179\n",
      "    Class 12       0.89      0.95      0.92       249\n",
      "    Class 13       0.88      0.94      0.91       188\n",
      "    Class 14       0.97      0.96      0.96       190\n",
      "    Class 15       0.97      0.91      0.94       213\n",
      "    Class 16       0.94      0.86      0.90       177\n",
      "    Class 17       0.94      0.94      0.94       174\n",
      "    Class 18       0.94      0.91      0.92       202\n",
      "    Class 19       0.90      0.95      0.92       200\n",
      "    Class 20       0.86      0.94      0.89       204\n",
      "    Class 21       0.96      0.91      0.94       193\n",
      "    Class 22       0.90      0.95      0.93       194\n",
      "    Class 23       0.93      0.91      0.92       191\n",
      "    Class 24       0.92      0.92      0.92       190\n",
      "    Class 25       0.91      0.94      0.92       213\n",
      "    Class 26       0.95      0.93      0.94       181\n",
      "\n",
      "    accuracy                           0.92      5353\n",
      "   macro avg       0.93      0.92      0.92      5353\n",
      "weighted avg       0.93      0.92      0.92      5353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2466\n",
      "Validation Accuracy: 0.9324\n",
      "Validation F1 Score: 0.9331\n",
      "Confusion Matrix:\n",
      "[[179   4   0   0   0   0   0   1   0   0   3   1   0   3   0   0   0   0\n",
      "    0   1   0   0   0   0   0   0   0]\n",
      " [  4 177   0   0   0   1   0   0   0   0   1   0   0   0   2   1   0   0\n",
      "    0   1   0   1   0   0   0   3   0]\n",
      " [  1   2 204   0   1   0   0   4   0   0   2   1   0   0   0   0   0   0\n",
      "    0   1   0   0   0   3   0   0   0]\n",
      " [  1   0   0 190   0   1   0   1   0   0   3   2   0   1   0   0   0   0\n",
      "    3   0   0   0   0   0   0   0   1]\n",
      " [  1   0   1   0 167   0   0   2   0   3   4   0   0   2   1   8   0   0\n",
      "    0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0 201   0   0   0   2   1   0   0   0   3   0   0   0\n",
      "    2   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0 173   1   0   2   1   0   0   2   0   0   1   1\n",
      "    0   0   0   1   2   0   1   0   4]\n",
      " [  1   0   0   0   0   0   1 181   1   1   3   0   0   0   0   0   0   0\n",
      "    0   1   0   1   1   0   0   0   0]\n",
      " [  0   0   2   0   0   0   0   0 198   4   0   0   0   2   0   0   0   0\n",
      "    0   0   0   1   2   0   1   0   0]\n",
      " [  1   0   0   0   0   0   0   4   0 202   0   0   0   0   0   0   0   0\n",
      "    1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   3   0   0   0   0   0 200   0   0   0   0   0   0   0\n",
      "    1   1   1   0   0   0   4   0   0]\n",
      " [  2   0   0   0   0   0   0   0   1   0   5 163   0   0   2   0   0   0\n",
      "    0   1   1   0   0   0   4   0   0]\n",
      " [  1   1   0   0   0   0   1   1   0   1   2   0 230   5   2   0   0   1\n",
      "    1   1   0   0   1   0   1   0   0]\n",
      " [  1   0   0   1   0   0   0   1   0   2   0   1   1 180   0   0   0   0\n",
      "    1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   2   0   1   2   0   0   0 184   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   6   0   1   3   0   0   0   0   0   0   1 199   0   0\n",
      "    1   0   0   0   1   0   0   0   1]\n",
      " [  0   0   0   0   0   0   2   4   1   2   7   0   0   1   0   0 149   0\n",
      "    1   0   0   1   2   0   0   0   7]\n",
      " [  1   0   0   0   0   0   0   3   0   0   2   0   0   0   0   0   0 164\n",
      "    0   0   0   0   3   0   1   0   0]\n",
      " [  1   0   0   0   0   2   0   1   0   0   3   0   0   0   0   0   0   0\n",
      "  187   0   0   6   1   0   1   0   0]\n",
      " [  1   0   0   0   0   0   0   2   0   0   1   0   0   1   0   0   0   0\n",
      "    0 190   0   0   3   0   1   1   0]\n",
      " [  0   0   0   1   1   1   0   2   0   0   3   0   0   0   2   1   0   0\n",
      "    1   0 190   0   1   0   1   0   0]\n",
      " [  1   0   0   0   0   0   0   1   1   6   4   0   0   1   0   0   0   0\n",
      "    1   0   0 176   1   0   1   0   0]\n",
      " [  1   0   0   0   0   0   0   2   0   1   2   1   0   0   0   0   0   1\n",
      "    0   1   0   0 185   0   0   0   0]\n",
      " [  0   0   3   0   0   0   0   1   0   3   5   1   2   0   0   1   0   0\n",
      "    0   0   0   0   0 174   0   1   0]\n",
      " [  2   0   1   0   0   0   0   1   0   1   5   0   0   0   1   0   0   0\n",
      "    2   1   0   0   0   0 175   1   0]\n",
      " [  3   1   0   0   1   1   0   1   0   0   2   1   0   1   0   0   0   0\n",
      "    0   0   0   0   1   0   0 201   0]\n",
      " [  0   0   0   0   0   1   1   1   0   0   0   0   0   1   0   0   1   0\n",
      "    0   0   0   0   2   0   1   1 172]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.89      0.93      0.91       192\n",
      "     Class 1       0.95      0.93      0.94       191\n",
      "     Class 2       0.96      0.93      0.94       219\n",
      "     Class 3       0.99      0.94      0.96       203\n",
      "     Class 4       0.93      0.87      0.90       191\n",
      "     Class 5       0.97      0.96      0.96       209\n",
      "     Class 6       0.97      0.91      0.94       190\n",
      "     Class 7       0.82      0.95      0.88       191\n",
      "     Class 8       0.98      0.94      0.96       210\n",
      "     Class 9       0.87      0.97      0.92       208\n",
      "    Class 10       0.77      0.95      0.85       211\n",
      "    Class 11       0.95      0.91      0.93       179\n",
      "    Class 12       0.99      0.92      0.95       249\n",
      "    Class 13       0.90      0.96      0.93       188\n",
      "    Class 14       0.93      0.97      0.95       190\n",
      "    Class 15       0.95      0.93      0.94       213\n",
      "    Class 16       0.99      0.84      0.91       177\n",
      "    Class 17       0.98      0.94      0.96       174\n",
      "    Class 18       0.93      0.93      0.93       202\n",
      "    Class 19       0.95      0.95      0.95       200\n",
      "    Class 20       0.99      0.93      0.96       204\n",
      "    Class 21       0.94      0.91      0.93       193\n",
      "    Class 22       0.90      0.95      0.93       194\n",
      "    Class 23       0.98      0.91      0.95       191\n",
      "    Class 24       0.90      0.92      0.91       190\n",
      "    Class 25       0.97      0.94      0.95       213\n",
      "    Class 26       0.93      0.95      0.94       181\n",
      "\n",
      "    accuracy                           0.93      5353\n",
      "   macro avg       0.94      0.93      0.93      5353\n",
      "weighted avg       0.94      0.93      0.93      5353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2662\n",
      "Validation Accuracy: 0.9318\n",
      "Validation F1 Score: 0.9322\n",
      "Confusion Matrix:\n",
      "[[174   4   0   0   0   0   0   1   0   0   1   2   3   0   0   0   0   0\n",
      "    0   2   0   0   0   0   3   1   1]\n",
      " [  5 172   1   0   0   0   0   0   0   0   0   0   3   0   2   1   0   0\n",
      "    0   1   0   0   0   0   1   5   0]\n",
      " [  0   0 208   0   0   0   0   0   1   0   2   2   1   0   1   0   0   1\n",
      "    0   0   1   0   0   2   0   0   0]\n",
      " [  0   0   0 190   0   1   1   1   0   0   0   0   2   2   0   0   1   0\n",
      "    1   0   0   0   2   0   2   0   0]\n",
      " [  0   0   2   0 175   0   0   1   1   0   3   0   0   2   0   1   0   0\n",
      "    0   1   0   0   1   0   3   0   1]\n",
      " [  0   0   0   0   0 203   0   0   1   0   0   0   0   0   0   1   0   0\n",
      "    1   0   1   0   0   0   1   0   1]\n",
      " [  0   0   0   0   0   1 174   1   2   0   0   0   2   2   0   0   1   2\n",
      "    0   1   0   0   1   0   1   0   2]\n",
      " [  1   0   1   0   0   0   1 178   5   0   0   0   0   0   0   0   0   0\n",
      "    0   1   2   0   0   0   2   0   0]\n",
      " [  0   0   1   0   0   0   0   1 203   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1   2   1   1   0   0]\n",
      " [  1   0   0   0   0   0   0   3   1 200   1   1   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   1   1   2   0   0   0 189   0   1   0   0   0   0   0\n",
      "    0   1   2   0   0   0  13   0   1]\n",
      " [  1   1   0   0   0   0   0   0   1   0   1 164   0   0   0   0   0   2\n",
      "    0   0   0   0   0   1   8   0   0]\n",
      " [  0   0   0   0   0   0   1   1   0   0   0   1 237   2   1   0   0   2\n",
      "    0   2   1   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   1   2   0   1   1   1 180   0   0   0   0\n",
      "    0   0   0   0   0   1   1   0   0]\n",
      " [  0   0   1   0   0   0   0   0   1   0   1   0   0   0 183   0   0   0\n",
      "    0   0   3   0   0   0   1   0   0]\n",
      " [  0   0   0   0  13   1   2   2   0   0   0   0   0   0   1 192   0   1\n",
      "    0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0  10   1   5   0   3   1   0   0   0   0 147   2\n",
      "    1   0   1   0   1   0   1   0   4]\n",
      " [  0   0   0   0   0   0   1   0   0   0   1   0   0   0   0   0   0 166\n",
      "    0   0   1   0   1   0   3   0   1]\n",
      " [  0   0   0   0   0   2   0   0   3   0   1   0   0   0   0   1   0   1\n",
      "  187   0   2   2   0   0   2   1   0]\n",
      " [  0   0   0   0   0   0   0   1   1   0   0   0   1   3   0   0   0   1\n",
      "    0 191   1   0   0   0   1   0   0]\n",
      " [  0   0   1   0   0   0   0   1   0   0   1   1   3   1   1   1   0   1\n",
      "    0   0 192   0   1   0   0   0   0]\n",
      " [  1   0   1   0   0   1   2   1   2   2   1   0   0   1   0   1   0   0\n",
      "    3   1   0 172   0   0   4   0   0]\n",
      " [  0   0   0   0   0   1   0   4   0   0   0   0   0   0   0   0   0   1\n",
      "    0   3   2   0 183   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   2   2   0   0   1   4   2   0   2   0   0\n",
      "    0   0   1   0   1 175   0   0   0]\n",
      " [  1   1   1   0   0   1   0   0   1   0   1   1   0   1   0   0   0   2\n",
      "    0   1   1   1   0   0 177   0   0]\n",
      " [  1   0   0   0   0   1   0   1   0   0   2   1   0   1   0   0   0   1\n",
      "    0   0   0   0   0   0   0 205   0]\n",
      " [  0   0   0   0   0   2   4   0   0   0   0   0   1   1   0   0   0   1\n",
      "    0   0   0   0   0   0   0   1 171]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.91      0.92       192\n",
      "     Class 1       0.97      0.90      0.93       191\n",
      "     Class 2       0.96      0.95      0.95       219\n",
      "     Class 3       1.00      0.94      0.97       203\n",
      "     Class 4       0.92      0.92      0.92       191\n",
      "     Class 5       0.94      0.97      0.96       209\n",
      "     Class 6       0.88      0.92      0.90       190\n",
      "     Class 7       0.89      0.93      0.91       191\n",
      "     Class 8       0.88      0.97      0.92       210\n",
      "     Class 9       0.99      0.96      0.98       208\n",
      "    Class 10       0.90      0.90      0.90       211\n",
      "    Class 11       0.93      0.92      0.92       179\n",
      "    Class 12       0.92      0.95      0.93       249\n",
      "    Class 13       0.91      0.96      0.93       188\n",
      "    Class 14       0.97      0.96      0.97       190\n",
      "    Class 15       0.96      0.90      0.93       213\n",
      "    Class 16       0.99      0.83      0.90       177\n",
      "    Class 17       0.90      0.95      0.93       174\n",
      "    Class 18       0.97      0.93      0.95       202\n",
      "    Class 19       0.93      0.95      0.94       200\n",
      "    Class 20       0.91      0.94      0.93       204\n",
      "    Class 21       0.98      0.89      0.93       193\n",
      "    Class 22       0.94      0.94      0.94       194\n",
      "    Class 23       0.97      0.92      0.94       191\n",
      "    Class 24       0.78      0.93      0.85       190\n",
      "    Class 25       0.96      0.96      0.96       213\n",
      "    Class 26       0.94      0.94      0.94       181\n",
      "\n",
      "    accuracy                           0.93      5353\n",
      "   macro avg       0.93      0.93      0.93      5353\n",
      "weighted avg       0.93      0.93      0.93      5353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2905\n",
      "Validation Accuracy: 0.9326\n",
      "Validation F1 Score: 0.9334\n",
      "Confusion Matrix:\n",
      "[[176   4   0   0   0   0   0   1   0   0   0   4   0   0   0   0   0   2\n",
      "    0   2   0   0   1   0   0   2   0]\n",
      " [  6 175   0   0   0   0   0   0   0   0   0   2   0   0   2   1   0   0\n",
      "    0   2   0   0   0   0   1   2   0]\n",
      " [  0   3 200   0   0   0   0   0   1   1   0   3   0   1   1   0   0   3\n",
      "    0   0   0   0   0   6   0   0   0]\n",
      " [  0   0   0 194   0   0   0   2   0   0   0   0   1   0   0   0   1   2\n",
      "    0   0   1   0   1   0   0   0   1]\n",
      " [  1   1   0   1 169   0   0   2   0   0   0   0   0   0   1   7   0   4\n",
      "    0   0   0   0   1   1   1   1   1]\n",
      " [  1   0   0   0   0 202   0   0   0   1   0   1   0   0   0   0   0   1\n",
      "    1   0   0   0   0   0   1   0   1]\n",
      " [  1   2   0   1   0   0 172   1   0   0   0   0   1   1   0   1   1   3\n",
      "    0   0   0   0   3   0   0   0   3]\n",
      " [  2   0   0   0   0   0   1 179   3   0   0   2   0   0   0   0   0   1\n",
      "    0   0   0   1   0   0   2   0   0]\n",
      " [  1   0   0   0   0   0   0   1 204   0   0   0   0   1   0   0   0   0\n",
      "    0   0   0   1   1   1   0   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0 202   0   1   0   0   0   0   0   2\n",
      "    0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0   0 192   2   0   0   0   0   0   4\n",
      "    0   0   3   0   1   0   7   0   0]\n",
      " [  1   1   0   0   0   0   0   0   0   0   3 168   0   0   0   0   0   3\n",
      "    0   0   0   0   0   0   3   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   1 233   3   0   0   1   6\n",
      "    0   1   0   0   1   0   0   0   1]\n",
      " [  0   0   0   2   0   0   0   1   1   1   0   2   0 177   0   0   0   0\n",
      "    0   1   1   0   0   2   0   0   0]\n",
      " [  1   0   0   0   0   0   0   1   1   0   1   1   0   0 184   0   0   0\n",
      "    0   0   1   0   0   0   0   0   0]\n",
      " [  2   0   0   0   8   0   0   2   0   0   0   0   0   0   1 196   1   2\n",
      "    0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   1   0   0   1   2   1   0   1   3   0   0   0   0 152   6\n",
      "    0   0   1   0   2   0   1   0   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0 170\n",
      "    0   0   0   0   2   0   0   0   1]\n",
      " [  1   1   0   0   0   1   0   0   0   0   1   2   0   1   0   0   0   5\n",
      "  183   0   1   4   1   0   0   0   1]\n",
      " [  1   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "    0 190   0   0   2   0   0   1   0]\n",
      " [  0   2   0   0   0   0   0   1   0   0   0   1   0   2   2   0   0   1\n",
      "    0   0 193   0   2   0   0   0   0]\n",
      " [  1   0   1   2   0   0   1   0   0   3   0   0   0   1   0   0   1   5\n",
      "    0   0   0 173   2   0   3   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   1   0   1   0   0   0   0   0   3\n",
      "    0   3   0   0 185   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   2   0   0   1   2   2   3   0   1   0   3\n",
      "    0   0   0   0   1 175   0   0   0]\n",
      " [  1   2   0   0   0   0   0   2   0   0   0   2   0   0   3   0   0   4\n",
      "    0   1   0   0   0   0 175   0   0]\n",
      " [  2   1   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0   2\n",
      "    0   0   0   2   1   0   0 202   1]\n",
      " [  0   0   0   1   0   1   1   1   0   0   0   0   1   0   0   0   1   1\n",
      "    0   0   0   0   1   0   0   2 171]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.88      0.92      0.90       192\n",
      "     Class 1       0.90      0.92      0.91       191\n",
      "     Class 2       1.00      0.91      0.95       219\n",
      "     Class 3       0.96      0.96      0.96       203\n",
      "     Class 4       0.94      0.88      0.91       191\n",
      "     Class 5       0.99      0.97      0.98       209\n",
      "     Class 6       0.98      0.91      0.94       190\n",
      "     Class 7       0.89      0.94      0.91       191\n",
      "     Class 8       0.97      0.97      0.97       210\n",
      "     Class 9       0.97      0.97      0.97       208\n",
      "    Class 10       0.96      0.91      0.94       211\n",
      "    Class 11       0.84      0.94      0.89       179\n",
      "    Class 12       0.98      0.94      0.96       249\n",
      "    Class 13       0.93      0.94      0.93       188\n",
      "    Class 14       0.95      0.97      0.96       190\n",
      "    Class 15       0.95      0.92      0.94       213\n",
      "    Class 16       0.96      0.86      0.91       177\n",
      "    Class 17       0.72      0.98      0.83       174\n",
      "    Class 18       0.99      0.91      0.95       202\n",
      "    Class 19       0.95      0.95      0.95       200\n",
      "    Class 20       0.96      0.95      0.95       204\n",
      "    Class 21       0.96      0.90      0.93       193\n",
      "    Class 22       0.89      0.95      0.92       194\n",
      "    Class 23       0.95      0.92      0.93       191\n",
      "    Class 24       0.90      0.92      0.91       190\n",
      "    Class 25       0.96      0.95      0.96       213\n",
      "    Class 26       0.91      0.94      0.93       181\n",
      "\n",
      "    accuracy                           0.93      5353\n",
      "   macro avg       0.93      0.93      0.93      5353\n",
      "weighted avg       0.94      0.93      0.93      5353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2970\n",
      "Validation Accuracy: 0.9344\n",
      "Validation F1 Score: 0.9347\n",
      "Confusion Matrix:\n",
      "[[176   8   0   2   0   0   0   1   0   0   0   1   0   0   0   0   1   1\n",
      "    0   1   0   0   0   0   0   1   0]\n",
      " [  4 179   0   1   0   0   0   0   0   0   0   0   0   0   2   1   0   0\n",
      "    0   1   0   0   0   0   1   1   1]\n",
      " [  0   1 207   1   0   0   0   2   1   0   0   0   0   0   2   0   0   1\n",
      "    0   0   0   0   0   2   1   1   0]\n",
      " [  1   0   0 197   0   0   1   2   0   0   0   0   0   1   0   0   0   0\n",
      "    0   0   0   0   1   0   0   0   0]\n",
      " [  0   1   1   3 169   0   1   2   1   1   0   0   0   1   2   6   1   0\n",
      "    0   1   0   0   0   0   1   0   0]\n",
      " [  0   0   1   0   0 203   1   0   0   1   0   0   0   0   1   0   0   0\n",
      "    1   0   0   0   0   0   0   0   1]\n",
      " [  0   0   0   2   0   2 176   1   1   0   0   0   0   1   1   1   0   1\n",
      "    1   1   0   0   0   0   0   0   2]\n",
      " [  1   0   0   1   0   0   1 179   1   0   0   0   0   0   0   0   0   2\n",
      "    0   1   2   1   0   0   2   0   0]\n",
      " [  0   0   1   0   0   0   0   2 200   1   0   0   1   0   0   0   0   0\n",
      "    1   0   0   1   1   1   1   0   0]\n",
      " [  1   1   0   0   0   0   0   2   0 201   0   0   0   0   0   0   0   0\n",
      "    0   0   1   0   1   0   0   0   1]\n",
      " [  0   2   0   1   1   0   2   0   0   0 189   3   0   0   1   0   0   0\n",
      "    0   1   2   1   0   1   7   0   0]\n",
      " [  2   0   0   2   0   0   0   1   0   0   0 169   0   0   0   0   0   1\n",
      "    0   0   1   0   0   0   3   0   0]\n",
      " [  1   0   0   1   0   0   1   1   0   0   0   1 233   4   1   0   0   2\n",
      "    0   2   0   0   1   0   0   0   1]\n",
      " [  0   0   0   4   0   0   0   0   2   0   1   0   1 177   0   1   1   0\n",
      "    0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   1   1   0   0   0   0   0 184   0   0   0\n",
      "    0   1   2   0   0   0   0   0   0]\n",
      " [  2   0   0   0   5   1   2   2   0   0   0   0   0   0   1 200   0   0\n",
      "    0   0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   3   0   0   4   4   0   0   0   0   0   0   1   0 153   1\n",
      "    1   0   1   0   0   0   1   0   6]\n",
      " [  1   0   1   0   0   0   1   1   0   0   0   0   0   0   0   0   0 166\n",
      "    0   0   0   0   1   0   1   0   2]\n",
      " [  1   0   2   2   0   1   1   0   1   0   0   1   0   0   0   0   0   0\n",
      "  187   1   0   2   1   0   1   0   1]\n",
      " [  0   0   0   1   1   0   0   1   0   0   0   0   0   2   0   0   1   0\n",
      "    0 190   0   1   0   0   1   2   0]\n",
      " [  0   2   0   0   1   0   0   1   0   0   0   0   0   2   3   1   0   1\n",
      "    0   0 192   0   1   0   0   0   0]\n",
      " [  0   0   2   2   0   0   4   1   0   3   0   0   0   0   1   0   0   1\n",
      "    2   0   0 173   1   0   2   0   1]\n",
      " [  0   0   1   0   0   0   0   3   0   0   0   1   0   0   0   0   1   2\n",
      "    0   2   0   0 184   0   0   0   0]\n",
      " [  0   0   3   0   0   0   1   3   0   0   0   0   3   2   0   1   0   1\n",
      "    0   0   0   0   1 174   0   1   1]\n",
      " [  0   1   1   2   0   1   0   3   0   0   0   2   1   0   0   0   0   1\n",
      "    0   2   0   1   0   0 175   0   0]\n",
      " [  3   2   0   1   0   1   0   3   0   0   0   0   0   1   0   0   0   2\n",
      "    0   1   0   0   0   0   0 199   0]\n",
      " [  0   0   0   2   0   0   5   0   0   0   1   0   0   1   0   0   0   0\n",
      "    0   0   0   0   0   0   1   1 170]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.90      0.92      0.91       192\n",
      "     Class 1       0.91      0.94      0.92       191\n",
      "     Class 2       0.94      0.95      0.94       219\n",
      "     Class 3       0.86      0.97      0.91       203\n",
      "     Class 4       0.95      0.88      0.92       191\n",
      "     Class 5       0.97      0.97      0.97       209\n",
      "     Class 6       0.88      0.93      0.90       190\n",
      "     Class 7       0.83      0.94      0.88       191\n",
      "     Class 8       0.96      0.95      0.96       210\n",
      "     Class 9       0.97      0.97      0.97       208\n",
      "    Class 10       0.99      0.90      0.94       211\n",
      "    Class 11       0.95      0.94      0.95       179\n",
      "    Class 12       0.97      0.94      0.95       249\n",
      "    Class 13       0.92      0.94      0.93       188\n",
      "    Class 14       0.92      0.97      0.94       190\n",
      "    Class 15       0.95      0.94      0.94       213\n",
      "    Class 16       0.97      0.86      0.91       177\n",
      "    Class 17       0.91      0.95      0.93       174\n",
      "    Class 18       0.97      0.93      0.95       202\n",
      "    Class 19       0.93      0.95      0.94       200\n",
      "    Class 20       0.95      0.94      0.95       204\n",
      "    Class 21       0.96      0.90      0.93       193\n",
      "    Class 22       0.95      0.95      0.95       194\n",
      "    Class 23       0.98      0.91      0.94       191\n",
      "    Class 24       0.88      0.92      0.90       190\n",
      "    Class 25       0.97      0.93      0.95       213\n",
      "    Class 26       0.91      0.94      0.92       181\n",
      "\n",
      "    accuracy                           0.93      5353\n",
      "   macro avg       0.94      0.93      0.93      5353\n",
      "weighted avg       0.94      0.93      0.93      5353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the dataset from CSV\n",
    "def load_dataset(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if 'instruction_augmented' not in df.columns or 'intent' not in df.columns:\n",
    "        raise ValueError(\"Dataset must contain 'instruction_augmented' and 'intent' columns\")\n",
    "    return df\n",
    "\n",
    "csv_file = \"augmented_data4.csv\"\n",
    "print(\"Loading dataset...\")\n",
    "df = load_dataset(csv_file)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# --- Analysis before dropping ---\n",
    "print(\"\\n--- Before Dropping ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing values in 'instruction_augmented': {df['instruction_augmented'].isnull().sum()}\")\n",
    "\n",
    "# --- Drop missing values ---\n",
    "df.dropna(subset=['instruction_augmented'], inplace=True)\n",
    "\n",
    "# --- Analysis after dropping ---\n",
    "print(\"\\n--- After Dropping ---\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Missing values in 'instruction_augmented': {df['instruction_augmented'].isnull().sum()}\")\n",
    "\n",
    "# --- Validation ---\n",
    "if df['instruction_augmented'].isnull().sum() == 0:\n",
    "    print(\"\\nValidation: No missing values in 'instruction_augmented' after dropping.\")\n",
    "else:\n",
    "    print(\"\\nValidation: There are still missing values in 'instruction_augmented'.\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n",
    "\n",
    "# Load pre-trained DistilBert tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load DistilBert model for sequence classification\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['intent'].unique()))\n",
    "\n",
    "# Move the model to the GPU if available, otherwise, use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "distilbert_model.to(device)\n",
    "\n",
    "# Get the number of unique intents\n",
    "num_intents = len(df['intent'].unique())\n",
    "print(f\"Number of unique intents: {num_intents}\")\n",
    "\n",
    "# Encode labels for intent classification only\n",
    "def encode_labels(df):\n",
    "    # Create a mapping for intent labels\n",
    "    intent_map = {intent: idx for idx, intent in enumerate(df['intent'].unique())}\n",
    "\n",
    "    # Reverse mapping for decoding (optional)\n",
    "    reverse_intent_map = {v: k for k, v in intent_map.items()}\n",
    "\n",
    "    # Encode 'intent' column\n",
    "    df['intent_encoded'] = df['intent'].map(intent_map)\n",
    "\n",
    "    return df, intent_map, reverse_intent_map\n",
    "\n",
    "# Encode labels\n",
    "print(\"Encoding labels...\")\n",
    "df, intent_map, reverse_intent_map = encode_labels(df)\n",
    "\n",
    "# Print intent mappings\n",
    "print(\"\\nIntent Mapping:\")\n",
    "print(intent_map)\n",
    "\n",
    "# Save the preprocessed dataset as JSON\n",
    "def save_preprocessed_dataset(df, json_file):\n",
    "    # Convert DataFrame to JSON format\n",
    "    df.to_json(json_file, orient='records', lines=True)\n",
    "\n",
    "# Path to save the preprocessed JSON file\n",
    "json_file = \"preprocessed_dataset.json\"  # Replace with your desired output path\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "print(\"\\nSaving preprocessed dataset as JSON...\")\n",
    "save_preprocessed_dataset(df, json_file)\n",
    "print(f\"Preprocessed dataset saved to {json_file}\")\n",
    "\n",
    "# Load the preprocessed dataset from JSON for intent classification only\n",
    "print(\"Loading preprocessed dataset from JSON...\")\n",
    "preprocessed_df = pd.read_json(json_file, lines=True)\n",
    "\n",
    "# Display the first few rows of the preprocessed dataset\n",
    "print(\"\\nPreprocessed Dataset:\")\n",
    "print(preprocessed_df.head())\n",
    "\n",
    "# Verify the encoded 'intent' column\n",
    "print(\"\\nEncoded Intent Example:\")\n",
    "print(preprocessed_df[['instruction_augmented', 'intent', 'intent_encoded']].head())\n",
    "\n",
    "# Define a custom dataset class for intent classification\n",
    "class IntentClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['instruction_augmented']\n",
    "        intent_label = row['intent_encoded']\n",
    "\n",
    "        # Tokenize the input\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'intent_labels': intent_label\n",
    "        }\n",
    "\n",
    "# Create datasets (train/test split)\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = IntentClassificationDataset(train_data, tokenizer, max_length=128)\n",
    "val_dataset = IntentClassificationDataset(val_data, tokenizer, max_length=128)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights\n",
    "intent_weights = compute_class_weight('balanced', classes=np.unique(df['intent_encoded']), y=df['intent_encoded'])\n",
    "intent_weights = torch.tensor(intent_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define loss function\n",
    "intent_loss_fn = nn.CrossEntropyLoss(weight=intent_weights)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(distilbert_model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training function\n",
    "def train_model(dataloader, model, optimizer, intent_loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        intent_labels = batch['intent_labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=intent_labels)\n",
    "        intent_loss = outputs.loss\n",
    "\n",
    "        intent_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += intent_loss.item()\n",
    "        progress_bar.set_postfix(loss=intent_loss.item())\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(dataloader, model, intent_loss_fn, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            intent_labels = batch['intent_labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=intent_labels)\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            labels = intent_labels.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[f\"Class {i}\" for i in range(num_intents)])\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(dataloader)\n",
    "    return accuracy, f1, avg_val_loss, conf_matrix, report\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss = train_model(train_dataloader, distilbert_model, optimizer, intent_loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    val_accuracy, val_f1, val_loss, val_conf_matrix, val_report = evaluate_model(val_dataloader, distilbert_model, intent_loss_fn, device)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(val_conf_matrix)\n",
    "    print(\"Classification Report:\")\n",
    "    print(val_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
