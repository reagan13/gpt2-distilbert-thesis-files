{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "gbKval_1PJak"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reagan13/gpt2-distilbert-thesis-files/blob/main/notebook/Hybrid_Model_WeightedSum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multitask Learning with Hybrid (GPT2-Distilbert)"
      ],
      "metadata": {
        "id": "WU5-iqe8XY46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "uDdgWxvqUIgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Optional\n",
        "from collections import Counter, defaultdict\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    GPT2Model, GPT2Config, GPT2TokenizerFast,\n",
        "    DistilBertModel, DistilBertTokenizerFast,  # Added for DistilBERT\n",
        "    AdamW, get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "from datetime import datetime\n",
        "\n",
        "# Verify device and GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Selected device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Initial GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"No GPU detected. Running on CPU.\")\n",
        "\n",
        "# Function to check device of tensors or models (retained)\n",
        "def check_device(item, name=\"Item\"):\n",
        "    if isinstance(item, torch.nn.Module):\n",
        "        param = next(item.parameters(), None)\n",
        "        if param is not None:\n",
        "            print(f\"{name} is on: {param.device}\")\n",
        "        else:\n",
        "            print(f\"{name} has no parameters to check\")\n",
        "    elif isinstance(item, torch.Tensor):\n",
        "        print(f\"{name} is on: {item.device}\")\n",
        "    else:\n",
        "        print(f\"{name} is not a tensor or model: {type(item)}\")\n",
        "\n",
        "def setup_logging(save_path: str, filename: str = \"training_log.txt\"):\n",
        "    try:\n",
        "        # Ensure local directory exists\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        local_log_path = os.path.join(save_path, filename)\n",
        "        print(f\"Local log path created: {local_log_path}\")\n",
        "\n",
        "        # Ensure Google Drive directory exists\n",
        "        drive_base_path = '/content/drive/'\n",
        "        if not os.path.exists(drive_base_path):\n",
        "            raise FileNotFoundError(\"Google Drive not mounted. Please mount it first.\")\n",
        "        drive_path = os.path.join(drive_base_path, save_path)\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "        drive_log_path = os.path.join(drive_path, filename)\n",
        "        print(f\"Google Drive log path created: {drive_log_path}\")\n",
        "\n",
        "        class Logger:\n",
        "            def __init__(self, local_path, drive_path):\n",
        "                # Open files in append mode with no buffering\n",
        "                self.local_file = open(local_path, \"a\", encoding=\"utf-8\", buffering=1)  # Line-buffered\n",
        "                self.drive_file = open(drive_path, \"a\", encoding=\"utf-8\", buffering=1)  # Line-buffered\n",
        "                self.original_stdout = sys.stdout\n",
        "\n",
        "            def write(self, message):\n",
        "                # Write to both files and original stdout\n",
        "                self.local_file.write(message)\n",
        "                self.drive_file.write(message)\n",
        "                self.original_stdout.write(message)\n",
        "                # Force flush to ensure immediate write\n",
        "                self.local_file.flush()\n",
        "                self.drive_file.flush()\n",
        "                self.original_stdout.flush()\n",
        "\n",
        "            def flush(self):\n",
        "                self.local_file.flush()\n",
        "                self.drive_file.flush()\n",
        "                self.original_stdout.flush()\n",
        "\n",
        "            def close(self):\n",
        "                self.local_file.close()\n",
        "                self.drive_file.close()\n",
        "                sys.stdout = self.original_stdout\n",
        "\n",
        "        # Instantiate logger and redirect stdout\n",
        "        logger = Logger(local_log_path, drive_log_path)\n",
        "        sys.stdout = logger\n",
        "\n",
        "        # Initial log messages\n",
        "        print(f\"Logging started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Log file: {local_log_path}\")\n",
        "        print(f\"Drive log file: {drive_log_path}\")\n",
        "\n",
        "        return logger  # Return logger for manual control if needed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up logging: {e}\", file=sys.__stdout__)  # Log to original stdout on error\n",
        "        sys.stdout = sys.__stdout__  # Reset stdout on failure\n",
        "        return None\n",
        "\n",
        "\n",
        "print(f\"Current date: {datetime.now().strftime('%B %d, %Y')}\")"
      ],
      "metadata": {
        "id": "CKCve4yhUHS6",
        "outputId": "f03b8920-705a-4c4d-d1ce-5a4424d182ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device: cuda\n",
            "GPU Name: Tesla T4\n",
            "Initial GPU Memory Allocated: 0.00 MB\n",
            "Current date: March 07, 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dasaet Loading Functions"
      ],
      "metadata": {
        "id": "G9xMYvhFUMeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(json_file: str) -> List[Dict]:\n",
        "    \"\"\"Load dataset from a JSON file.\"\"\"\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def detect_labels(data: List[Dict]) -> Dict[str, Dict]:\n",
        "    \"\"\"Detect unique labels and create encoders for categories, intents, and NER tags.\"\"\"\n",
        "    start_time = time.time()\n",
        "    if not isinstance(data, list):\n",
        "        raise TypeError(\"Input 'data' must be a list of dictionaries\")\n",
        "    if not data:\n",
        "        return {\"category_encoder\": {}, \"intent_encoder\": {}, \"ner_label_encoder\": {\"O\": 0}}\n",
        "\n",
        "    unique_categories = set()\n",
        "    unique_intents = set()\n",
        "    unique_ner_labels = set([\"O\"])\n",
        "    missing_fields = defaultdict(int)\n",
        "    category_counts = Counter()\n",
        "    intent_counts = Counter()\n",
        "    ner_counts = Counter()\n",
        "\n",
        "    for i, sample in enumerate(data):\n",
        "        try:\n",
        "            category = sample[\"category\"]\n",
        "            intent = sample[\"intent\"]\n",
        "            unique_categories.add(category)\n",
        "            unique_intents.add(intent)\n",
        "            category_counts[category] += 1\n",
        "            intent_counts[intent] += 1\n",
        "\n",
        "            ner_labels = sample[\"ner_labels_only\"]\n",
        "            if not isinstance(ner_labels, list):\n",
        "                raise ValueError(f\"'ner_labels_only' must be a list at sample {i}\")\n",
        "            for label in ner_labels:\n",
        "                if not isinstance(label, dict) or \"label\" not in label or \"text\" not in label:\n",
        "                    raise ValueError(f\"NER label must have 'label' and 'text' fields at sample {i}\")\n",
        "                label_type = label[\"label\"]\n",
        "                unique_ner_labels.add(f\"B-{label_type}\")\n",
        "                unique_ner_labels.add(f\"I-{label_type}\")\n",
        "                ner_counts[f\"B-{label_type}\"] += 1\n",
        "                ner_counts[f\"I-{label_type}\"] += 1\n",
        "        except KeyError as e:\n",
        "            missing_fields[str(e).strip(\"'\")] += 1\n",
        "            continue\n",
        "\n",
        "    if missing_fields:\n",
        "        print(\"Warning: Missing fields detected:\")\n",
        "        for field, count in missing_fields.items():\n",
        "            print(f\"  - '{field}' missing in {count} samples\")\n",
        "\n",
        "    category_encoder = {cat: idx for idx, cat in enumerate(sorted(unique_categories))}\n",
        "    intent_encoder = {intent: idx for idx, intent in enumerate(sorted(unique_intents))}\n",
        "    ner_label_encoder = {ner: idx for idx, ner in enumerate(sorted(unique_ner_labels))}\n",
        "\n",
        "    print(f\"Dataset summary:\\n  - {len(data)} samples\\n  - {len(category_encoder)} categories\\n  - {len(intent_encoder)} intents\\n  - {len(ner_label_encoder)} NER tags\")\n",
        "    print(\"Category distribution:\", dict(category_counts))\n",
        "    print(\"Intent distribution:\", dict(intent_counts))\n",
        "    print(\"NER tag distribution (non-O):\", dict(ner_counts))\n",
        "    print(f\"Processing time: {time.time() - start_time:.3f} seconds\")\n",
        "\n",
        "    return {\"category_encoder\": category_encoder, \"intent_encoder\": intent_encoder, \"ner_label_encoder\": ner_label_encoder}"
      ],
      "metadata": {
        "id": "NQhYy09xUPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and NER Alignment"
      ],
      "metadata": {
        "id": "ZSJYDchMUT20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text_hybrid(text: str, gpt2_tokenizer, distilbert_tokenizer, max_length: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Tokenize text using both GPT-2 and DistilBERT tokenizers.\"\"\"\n",
        "    gpt2_inputs = gpt2_tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    distilbert_inputs = distilbert_tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"gpt2_input_ids\": gpt2_inputs[\"input_ids\"].squeeze(0),\n",
        "        \"gpt2_attention_mask\": gpt2_inputs[\"attention_mask\"].squeeze(0),\n",
        "        \"distilbert_input_ids\": distilbert_inputs[\"input_ids\"].squeeze(0),\n",
        "        \"distilbert_attention_mask\": distilbert_inputs[\"attention_mask\"].squeeze(0)\n",
        "    }\n",
        "\n",
        "def align_ner_labels(text: str, ner_labels: List[Dict], tokenizer, ner_label_encoder: Dict, max_length: int) -> torch.Tensor:\n",
        "    \"\"\"Align NER labels with tokenized input (using GPT-2 tokenizer for consistency).\"\"\"\n",
        "    sorted_labels = sorted(ner_labels, key=lambda x: len(x[\"text\"]), reverse=True) if ner_labels else []\n",
        "    encoding = tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_offsets_mapping=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    token_to_char_map = encoding[\"offset_mapping\"][0].tolist()\n",
        "    ner_aligned = [ner_label_encoder[\"O\"]] * max_length\n",
        "\n",
        "    for label in sorted_labels:\n",
        "        if \"text\" not in label or \"label\" not in label:\n",
        "            print(f\"Warning: Skipping invalid NER entry {label} (missing 'text' or 'label')\")\n",
        "            continue\n",
        "        try:\n",
        "            label_text, label_type = label[\"text\"], label[\"label\"]\n",
        "            start_pos = 0\n",
        "            while True:\n",
        "                label_start = text.find(label_text, start_pos)\n",
        "                if label_start == -1:\n",
        "                    break\n",
        "                label_end = label_start + len(label_text)\n",
        "                start_pos = label_end\n",
        "                first_token = True\n",
        "                for i, (start, end) in enumerate(token_to_char_map):\n",
        "                    if start == 0 and end == 0:\n",
        "                        continue\n",
        "                    if max(start, label_start) < min(end, label_end):\n",
        "                        prefix = \"B-\" if first_token else \"I-\"\n",
        "                        first_token = False\n",
        "                        ner_aligned[i] = ner_label_encoder.get(f\"{prefix}{label_type}\", ner_label_encoder[\"O\"])\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Label '{e}' not found in encoder. Skipping.\")\n",
        "\n",
        "    return torch.tensor(ner_aligned, dtype=torch.long)"
      ],
      "metadata": {
        "id": "9kMHmK7lUZUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Dataloader"
      ],
      "metadata": {
        "id": "ee_1s8_LUb-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data: List[Dict], gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length: int):\n",
        "        self.data = data\n",
        "        self.gpt2_tokenizer = gpt2_tokenizer\n",
        "        self.distilbert_tokenizer = distilbert_tokenizer\n",
        "        self.label_encoders = label_encoders\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        text = sample[\"instruction\"]\n",
        "         # Tokenize with GPT-2\n",
        "        gpt2_inputs = self.gpt2_tokenizer(\n",
        "            text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        # Tokenize with DistilBERT\n",
        "        distilbert_inputs = self.distilbert_tokenizer(\n",
        "            text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Align NER labels with GPT-2 tokenization (consistent with baseline)\n",
        "        ner_labels = align_ner_labels(text, sample[\"ner_labels_only\"], self.gpt2_tokenizer,\n",
        "                                     self.label_encoders[\"ner_label_encoder\"], self.max_length)\n",
        "        return {\n",
        "            \"gpt2_input_ids\": hybrid_inputs[\"gpt2_input_ids\"],\n",
        "            \"gpt2_attention_mask\": hybrid_inputs[\"gpt2_attention_mask\"],\n",
        "            \"distilbert_input_ids\": hybrid_inputs[\"distilbert_input_ids\"],\n",
        "            \"distilbert_attention_mask\": hybrid_inputs[\"distilbert_attention_mask\"],\n",
        "            \"category_labels\": torch.tensor(self.label_encoders[\"category_encoder\"][sample[\"category\"]], dtype=torch.long),\n",
        "            \"intent_labels\": torch.tensor(self.label_encoders[\"intent_encoder\"][sample[\"intent\"]], dtype=torch.long),\n",
        "            \"ner_labels\": ner_labels\n",
        "        }\n",
        "\n",
        "def get_dataloaders(train_data, val_data, test_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, batch_size, num_workers, max_length):\n",
        "    \"\"\"Create DataLoaders for train, validation, and test sets.\"\"\"\n",
        "    pin_memory = device.type == \"cuda\"\n",
        "    train_dataset = MultiTaskDataset(train_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length)\n",
        "    val_dataset = MultiTaskDataset(val_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length)\n",
        "    test_dataset = MultiTaskDataset(test_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length)\n",
        "\n",
        "    return (\n",
        "        DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory),\n",
        "        DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory),\n",
        "        DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    )"
      ],
      "metadata": {
        "id": "wYt1cv-1UePb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "8_kvjRnHUfo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionFusionLayer(nn.Module):\n",
        "    def __init__(self, gpt2_dim: int, bert_dim: int, output_dim: int, dropout_rate: float, num_heads: int = 8):\n",
        "        super().__init__()\n",
        "        self.gpt2_proj = nn.Linear(gpt2_dim, output_dim)\n",
        "        self.bert_proj = nn.Linear(bert_dim, output_dim)\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim=output_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, gpt2_features: torch.Tensor, bert_features: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        gpt2_proj = self.gpt2_proj(gpt2_features)  # [batch_size, seq_len, output_dim]\n",
        "        bert_proj = self.bert_proj(bert_features)  # [batch_size, seq_len, output_dim]\n",
        "        attn_mask = attention_mask.float().masked_fill(attention_mask == 0, float('-inf')).masked_fill(attention_mask == 1, 0)\n",
        "        fused_features, _ = self.cross_attention(\n",
        "            query=gpt2_proj,\n",
        "            key=bert_proj,\n",
        "            value=bert_proj,\n",
        "            key_padding_mask=attention_mask == 0\n",
        "        )\n",
        "        fused_features = self.dropout(fused_features) + gpt2_proj  # Residual connection\n",
        "        return self.layer_norm(fused_features)\n",
        "\n",
        "class HybridGPT2DistilBERTMultiTask(nn.Module):\n",
        "    def __init__(self, num_intents: int, num_categories: int, num_ner_labels: int, dropout_rate: float = 0.4):\n",
        "        super().__init__()\n",
        "        self.gpt2_config = GPT2Config.from_pretrained('gpt2')\n",
        "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        gpt2_dim = self.gpt2_config.n_embd\n",
        "        bert_dim = self.distilbert.config.hidden_size\n",
        "        hidden_size = gpt2_dim  # Keeping output dim same as GPT-2 for consistency\n",
        "\n",
        "        self.fusion_layer = CrossAttentionFusionLayer(gpt2_dim, bert_dim, hidden_size, dropout_rate)\n",
        "\n",
        "        self.intent_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_intents)\n",
        "        )\n",
        "        self.category_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_categories)\n",
        "        )\n",
        "        self.ner_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_ner_labels)\n",
        "        )\n",
        "\n",
        "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
        "        self.category_loss_fn = nn.CrossEntropyLoss()\n",
        "        self.ner_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, gpt2_input_ids: torch.Tensor, gpt2_attention_mask: torch.Tensor,\n",
        "                distilbert_input_ids: torch.Tensor, distilbert_attention_mask: torch.Tensor,\n",
        "                intent_labels: Optional[torch.Tensor] = None,\n",
        "                category_labels: Optional[torch.Tensor] = None,\n",
        "                ner_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        gpt2_outputs = self.gpt2(input_ids=gpt2_input_ids, attention_mask=gpt2_attention_mask)\n",
        "        distilbert_outputs = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distilbert_attention_mask)\n",
        "\n",
        "        gpt2_features = gpt2_outputs.last_hidden_state\n",
        "        bert_features = distilbert_outputs.last_hidden_state\n",
        "\n",
        "        fused_features = self.fusion_layer(gpt2_features, bert_features)\n",
        "\n",
        "        batch_size = fused_features.shape[0]\n",
        "        sequence_lengths = gpt2_attention_mask.sum(dim=1) - 1\n",
        "        last_token_indexes = sequence_lengths.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, fused_features.shape[-1])\n",
        "        sequence_repr = torch.gather(fused_features, 1, last_token_indexes).squeeze(1)\n",
        "\n",
        "        intent_logits = self.intent_head(sequence_repr)\n",
        "        category_logits = self.category_head(sequence_repr)\n",
        "        ner_logits = self.ner_head(fused_features)\n",
        "\n",
        "        output_dict = {\n",
        "            'intent_logits': intent_logits,\n",
        "            'category_logits': category_logits,\n",
        "            'ner_logits': ner_logits\n",
        "        }\n",
        "\n",
        "        if all(label is not None for label in [intent_labels, category_labels, ner_labels]):\n",
        "            intent_loss = self.intent_loss_fn(intent_logits, intent_labels)\n",
        "            category_loss = self.category_loss_fn(category_logits, category_labels)\n",
        "            active_loss = gpt2_attention_mask.view(-1) == 1\n",
        "            active_logits = ner_logits.view(-1, ner_logits.size(-1))[active_loss]\n",
        "            active_labels = ner_labels.view(-1)[active_loss]\n",
        "            ner_loss = self.ner_loss_fn(active_logits, active_labels)\n",
        "\n",
        "            output_dict.update({\n",
        "                'loss': intent_loss + category_loss + ner_loss,\n",
        "                'intent_loss': intent_loss,\n",
        "                'category_loss': category_loss,\n",
        "                'ner_loss': ner_loss\n",
        "            })\n",
        "\n",
        "        return output_dict"
      ],
      "metadata": {
        "id": "SQeWYh9uUg7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "DEhnmgZNUkU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
        "    \"\"\"Train the multi-task model.\"\"\"\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "    history = {\n",
        "        \"train_loss\": [], \"val_loss\": [],\n",
        "        \"train_intent_acc\": [], \"val_intent_acc\": [],\n",
        "        \"train_category_f1\": [], \"val_category_f1\": [],\n",
        "        \"train_ner_f1\": [], \"val_ner_f1\": []\n",
        "    }\n",
        "\n",
        "    model.to(device)\n",
        "    check_device(model, \"Model\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        all_train_intent_preds, all_train_intent_labels = [], []\n",
        "        all_train_category_preds, all_train_category_labels = [], []\n",
        "        all_train_ner_preds, all_train_ner_labels = [], []\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", leave=False) as train_loop:\n",
        "            for i, batch in enumerate(train_loop):\n",
        "                optimizer.zero_grad()\n",
        "                inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**inputs)\n",
        "                loss = outputs[\"loss\"]\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "                intent_preds = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().numpy()\n",
        "                category_preds = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().numpy()\n",
        "                ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()\n",
        "\n",
        "                all_train_intent_preds.extend(intent_preds)\n",
        "                all_train_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n",
        "                all_train_category_preds.extend(category_preds)\n",
        "                all_train_category_labels.extend(batch[\"category_labels\"].cpu().numpy())\n",
        "                all_train_ner_preds.extend(ner_preds.flatten())\n",
        "                all_train_ner_labels.extend(batch[\"ner_labels\"].cpu().numpy().flatten())\n",
        "\n",
        "        train_intent_acc = accuracy_score(all_train_intent_labels, all_train_intent_preds)\n",
        "        train_category_f1 = precision_recall_fscore_support(all_train_category_labels, all_train_category_preds, average=\"macro\", zero_division=0)[2]\n",
        "        train_ner_f1 = precision_recall_fscore_support(all_train_ner_labels, all_train_ner_preds, average=\"macro\", zero_division=0)[2]\n",
        "\n",
        "        history[\"train_loss\"].append(total_loss / len(train_loader))\n",
        "        history[\"train_intent_acc\"].append(train_intent_acc)\n",
        "        history[\"train_category_f1\"].append(train_category_f1)\n",
        "        history[\"train_ner_f1\"].append(train_ner_f1)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_intent_preds, all_val_intent_labels = [], []\n",
        "        all_val_category_preds, all_val_category_labels = [], []\n",
        "        all_val_ner_preds, all_val_ner_labels = [], []\n",
        "\n",
        "        val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loop:\n",
        "                inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**inputs)\n",
        "                batch_val_loss = outputs[\"loss\"].item()\n",
        "                val_loss += batch_val_loss\n",
        "                val_loop.set_postfix(loss=batch_val_loss)\n",
        "\n",
        "                intent_preds = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().numpy()\n",
        "                category_preds = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().numpy()\n",
        "                ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()\n",
        "\n",
        "                all_val_intent_preds.extend(intent_preds)\n",
        "                all_val_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n",
        "                all_val_category_preds.extend(category_preds)\n",
        "                all_val_category_labels.extend(batch[\"category_labels\"].cpu().numpy())\n",
        "                all_val_ner_preds.extend(ner_preds.flatten())\n",
        "                all_val_ner_labels.extend(batch[\"ner_labels\"].cpu().numpy().flatten())\n",
        "\n",
        "        val_intent_acc = accuracy_score(all_val_intent_labels, all_val_intent_preds)\n",
        "        val_category_f1 = precision_recall_fscore_support(all_val_category_labels, all_val_category_preds, average=\"macro\", zero_division=0)[2]\n",
        "        val_ner_f1 = precision_recall_fscore_support(all_val_ner_labels, all_val_ner_preds, average=\"macro\", zero_division=0)[2]\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss / len(val_loader))\n",
        "        history[\"val_intent_acc\"].append(val_intent_acc)\n",
        "        history[\"val_category_f1\"].append(val_category_f1)\n",
        "        history[\"val_ner_f1\"].append(val_ner_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train Loss:      {history['train_loss'][-1]:.4f}\")\n",
        "        print(f\"  Val Loss:       {history['val_loss'][-1]:.4f}\\n\")\n",
        "        print(f\"  Train Intent Acc: {train_intent_acc:.4f}\")\n",
        "        print(f\"  Val Intent Acc:  {val_intent_acc:.4f}\\n\")\n",
        "        print(f\"  Train Category F1:{train_category_f1:.4f}\")\n",
        "        print(f\"  Val Category F1: {val_category_f1:.4f}\\n\")\n",
        "        print(f\"  Train NER F1:     {train_ner_f1:.4f}\")\n",
        "        print(f\"  Val NER F1:      {val_ner_f1:.4f}\\n\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "rN3W8kTDUlTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "r_W1hO4EUmly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_intent_preds, all_intent_labels = [], []\n",
        "    all_category_preds, all_category_labels = [], []\n",
        "    all_ner_preds, all_ner_labels = [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    test_loop = tqdm(test_loader, desc=\"Evaluation\", leave=True)\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loop:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**inputs)\n",
        "            batch_loss = outputs[\"loss\"].item()\n",
        "            total_loss += batch_loss\n",
        "            test_loop.set_postfix(loss=batch_loss)\n",
        "\n",
        "            intent_preds = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().numpy()\n",
        "            category_preds = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().numpy()\n",
        "            ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()\n",
        "\n",
        "            all_intent_preds.extend(intent_preds)\n",
        "            all_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n",
        "            all_category_preds.extend(category_preds)\n",
        "            all_category_labels.extend(batch[\"category_labels\"].cpu().numpy())\n",
        "            all_ner_preds.extend(ner_preds.flatten())\n",
        "            all_ner_labels.extend(batch[\"ner_labels\"].cpu().numpy().flatten())\n",
        "\n",
        "    intent_acc = accuracy_score(all_intent_labels, all_intent_preds)\n",
        "    category_f1 = precision_recall_fscore_support(all_category_labels, all_category_preds, average=\"macro\", zero_division=0)[2]\n",
        "    ner_f1 = precision_recall_fscore_support(all_ner_labels, all_ner_preds, average=\"macro\", zero_division=0)[2]\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    results = {\n",
        "        \"loss\": avg_loss,\n",
        "        \"intent_accuracy\": intent_acc,\n",
        "        \"category_f1\": category_f1,\n",
        "        \"ner_f1\": ner_f1\n",
        "    }\n",
        "\n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"  Loss:            {avg_loss:.4f}\")\n",
        "    print(f\"  Intent Acc:      {intent_acc:.4f}\")\n",
        "    print(f\"  Category F1:     {category_f1:.4f}\")\n",
        "    print(f\"  NER F1:          {ner_f1:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "GvXcNp4vUoWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Artifacts"
      ],
      "metadata": {
        "id": "3TL8MuAeUrFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Locally"
      ],
      "metadata": {
        "id": "Z1zKgwNB84OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_artifacts(label_encoders, metrics, test_results, save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    label_encoders_path = os.path.join(save_path, \"label_encoders.json\")\n",
        "    with open(label_encoders_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(label_encoders, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Label encoders saved to {label_encoders_path}\")\n",
        "\n",
        "    training_metrics_path = os.path.join(save_path, \"training_metrics.json\")\n",
        "    with open(training_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Training metrics saved to {training_metrics_path}\")\n",
        "\n",
        "    test_results_path = os.path.join(save_path, \"test_results.json\")\n",
        "    with open(test_results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Test results saved to {test_results_path}\")\n",
        "\n",
        "def save_training_config(config, save_path, filename=\"training_config.json\"):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    config_path = os.path.join(save_path, filename)\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Training configuration saved to {config_path}\")\n",
        "\n",
        "def save_full_model(model, gpt2_tokenizer, distilbert_tokenizer, save_path):\n",
        "    model_path = os.path.join(save_path, \"model\")\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    model_file_path = os.path.join(model_path, \"full_model.pt\")\n",
        "    torch.save(model, model_file_path)\n",
        "    print(f\"Full model saved to {model_file_path}\")\n",
        "\n",
        "    gpt2_tokenizer_path = os.path.join(save_path, \"gpt2_tokenizer\")\n",
        "    gpt2_tokenizer.save_pretrained(gpt2_tokenizer_path)\n",
        "    print(f\"GPT-2 tokenizer saved to {gpt2_tokenizer_path}\")\n",
        "\n",
        "    distilbert_tokenizer_path = os.path.join(save_path, \"distilbert_tokenizer\")\n",
        "    distilbert_tokenizer.save_pretrained(distilbert_tokenizer_path)\n",
        "    print(f\"DistilBERT tokenizer saved to {distilbert_tokenizer_path}\")\n"
      ],
      "metadata": {
        "id": "NTNnt-hVUsle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save to GDrive"
      ],
      "metadata": {
        "id": "kRTW48OJ87HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution"
      ],
      "metadata": {
        "id": "D_mQwQdBU4QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "def save_training_config_to_drive(config, drive_path, filename=\"training_config.json\"):\n",
        "    os.makedirs(\"/content/drive/\" + drive_path, exist_ok=True)\n",
        "    config_path = os.path.join(\"/content/drive/\", drive_path, filename)\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Training configuration saved to {config_path}\")\n",
        "\n",
        "def save_artifacts_to_drive(label_encoders, metrics, test_results, drive_path):\n",
        "    os.makedirs(\"/content/drive/\" + drive_path, exist_ok=True)\n",
        "    label_encoders_path = os.path.join(\"/content/drive/\", drive_path, \"label_encoders.json\")\n",
        "    with open(label_encoders_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(label_encoders, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Label encoders saved to {label_encoders_path}\")\n",
        "\n",
        "    training_metrics_path = os.path.join(\"/content/drive/\", drive_path, \"training_metrics.json\")\n",
        "    with open(training_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Training metrics saved to {training_metrics_path}\")\n",
        "\n",
        "    test_results_path = os.path.join(\"/content/drive/\", drive_path, \"test_results.json\")\n",
        "    with open(test_results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(test_results, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Test results saved to {test_results_path}\")\n",
        "\n",
        "def save_full_model_to_drive(model, gpt2_tokenizer, distilbert_tokenizer, drive_path):\n",
        "    model_path = os.path.join(\"/content/drive/\", drive_path, \"model\")\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    model_file_path = os.path.join(model_path, \"full_model.pt\")\n",
        "    torch.save(model, model_file_path)\n",
        "    print(f\"Full model saved to {model_file_path}\")\n",
        "\n",
        "    gpt2_tokenizer_path = os.path.join(\"/content/drive/\", drive_path, \"gpt2_tokenizer\")\n",
        "    gpt2_tokenizer.save_pretrained(gpt2_tokenizer_path)\n",
        "    print(f\"GPT-2 tokenizer saved to {gpt2_tokenizer_path}\")\n",
        "\n",
        "    distilbert_tokenizer_path = os.path.join(\"/content/drive/\", drive_path, \"distilbert_tokenizer\")\n",
        "    distilbert_tokenizer.save_pretrained(distilbert_tokenizer_path)\n",
        "    print(f\"DistilBERT tokenizer saved to {distilbert_tokenizer_path}\")"
      ],
      "metadata": {
        "id": "lmb2KQ9y8-GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paths and Hyperparameters"
      ],
      "metadata": {
        "id": "7v48SU_2U6DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths and Hyperparameters\n",
        "train_file = \"train.json\"\n",
        "val_file = \"val.json\"\n",
        "test_file = \"test.json\"\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "max_length = 128\n",
        "num_workers = 2\n",
        "save_path = \"MyDrive/thesis/hybrid_cross_attention/test_1\"\n",
        "dropout_rate = 0.2\n",
        "\n",
        "training_config = {\n",
        "    \"train_file\": train_file,\n",
        "    \"val_file\": val_file,\n",
        "    \"test_file\": test_file,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"max_length\": max_length,\n",
        "    \"num_workers\": num_workers,\n",
        "    \"model_name\": \"HybridGPT2DistilBERTMultiTask\",\n",
        "    \"gpt2_base\": \"gpt2\",\n",
        "    \"distilbert_base\": \"distilbert-base-uncased\",\n",
        "    \"dropout_rate\": dropout_rate,\n",
        "    \"device\": str(device),\n",
        "    \"date\": datetime.now().strftime('%B %d, %Y')\n",
        "}\n"
      ],
      "metadata": {
        "id": "CiubTK2NU5h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "-pIstRkZU-Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "mount_drive()\n",
        "# Setup logging and keep logger in scope\n",
        "logger = setup_logging(save_path)\n",
        "if logger is None:\n",
        "    raise RuntimeError(\"Logging setup failed. Check Google Drive mount and permissions.\")\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading datasets...\\n\")\n",
        "train_data = load_dataset(train_file)\n",
        "val_data = load_dataset(val_file)\n",
        "test_data = load_dataset(test_file)\n",
        "\n",
        "print(\"*\" * 30)\n",
        "print(f\"\"\"Dataset Summary:\n",
        "Training samples: {len(train_data)}\n",
        "Validation samples: {len(val_data)}\n",
        "Test samples: {len(test_data)}\"\"\")\n",
        "\n",
        "# Detect labels\n",
        "label_encoders = detect_labels(train_data)\n",
        "\n",
        "# Initialize tokenizers\n",
        "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "if gpt2_tokenizer.pad_token is None:\n",
        "    gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader, test_loader = get_dataloaders(\n",
        "    train_data, val_data, test_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, batch_size, num_workers, max_length\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = HybridGPT2DistilBERTMultiTask(\n",
        "    num_intents=len(label_encoders[\"intent_encoder\"]),\n",
        "    num_categories=len(label_encoders[\"category_encoder\"]),\n",
        "    num_ner_labels=len(label_encoders[\"ner_label_encoder\"]),\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "if gpt2_tokenizer.pad_token_id is not None:\n",
        "    model.gpt2.resize_token_embeddings(len(gpt2_tokenizer))\n",
        "\n",
        "model.to(device)\n",
        "check_device(model, \"Model before training\")\n",
        "\n",
        "# Save training config\n",
        "save_training_config(training_config, save_path)\n",
        "\n",
        "# Train model\n",
        "print(\"*\" * 30)\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "metrics = train_model(model, train_loader, val_loader, num_epochs, learning_rate)\n",
        "print(f\"Training completed in {(time.time() - start_time) / 60:.2f} minutes\")\n",
        "print(\"*\" * 30)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating on test set...\")\n",
        "test_results = evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "gt_Gsk_LVAZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "06f8e6320b3548d79f0f08235cec5b49",
            "1dbdf7bf07f94049a9e81355b687d7e3",
            "78203ab295454f5f8f329d1a0c183652",
            "8b0bedd189ea4e28988bd3b413da0025",
            "4829e80d746e4203845bba1985212370",
            "b5603f5992544fb1aa78954445102ae7",
            "1b13db291842427eaf0eeb6275d19664",
            "6fe281b28f7546d6ae48a6c53bc6d995",
            "7b2faa9c4f384932b60e212c5ecbb250",
            "fd4e4cb8b957431fbe97af9688bc4362",
            "5b8ae2ad3ebd4b0d8790b8caf289e273",
            "ed105cef090a42d383db563ded9167c1",
            "21ae4f52bd814927b5071e0180c4d7ce",
            "3be90d366866437d85eefb15a66d88f0",
            "69d8233228894909a920cd19fcd8c48a",
            "f754804a48e44473b6d93cfd5de4a161",
            "389f3a25fc8a42c59d8f9f59732afe03",
            "19854f78d97d4499aa819e73f2b2bdff",
            "f395e5f073b24db380d236dd2ae9b1d8",
            "bb0ca2b3c39d4ea9bf2d2ccaa429b9fa",
            "cb9feebf501743cd89583fe78ad62e31",
            "23bb6732db83482887523c292a2ac87c",
            "1330ac25c24e48da8770f0b347a08e6c",
            "59b627e6490c46bfa6201e7cc9f710c6",
            "60f48e6e177043b1b89cf86bf3b67880",
            "d3fc7c79b9a64be88bbaff8b81c7c5c6",
            "fb7d3c526d4341be8ce13078b6b80f66",
            "81a5f4200d024c29acded947209077e7",
            "e21b354879d449f6b848d14dbb4b486e",
            "b3859baad68e4ca28938d1831dadc3ae",
            "470f9bb723c443f09b41a68c295f7a2e",
            "64e1a99c244044eba36d5ec8c0c2a03d",
            "de4a135776544eda8463366c14b8f515",
            "b4e4bc5209104cd1a2992d8d36932136",
            "d2a9f3ab5dbc4d3a87e18730c0e52bc2",
            "94baf5b739a845b0adf83a8189d0918a",
            "e9a32a3f3f494df0afce76a87e5fe245",
            "b5ea9bab549e4a51a103154b445c5774",
            "b0428e28a30a48cea336cd9669daa244",
            "46f58fb6a62246d6b59f78a75e0d5408",
            "c4b6cf21a5c242cea571d07912ec1491",
            "3c0ab3151d7f49948a7b98f94d7ec1fa",
            "16dc6d57e5464b319424cde99606d236",
            "779322f203d547a8a1ea02d98b14edbe",
            "7f53bd9554d340d0a19da3ad829677b5",
            "3a443570a8b04baa814c896d8a68ccaa",
            "f83deae20a584b608757d367729e373d",
            "c30edf2f83c04759bff6123d71ca7a7c",
            "7f39ba34aa6c4034af2ea0053427228f",
            "849cf5da5eac45b0872b64ede76b8098",
            "c37e13e05c144d82b6b1b420654e328d",
            "f54581766edf4126aceb8c23745c7fc6",
            "f9442128fcdb4ff38c2a44c64e5550ab",
            "8508d31830d44a7faae584dbade2468c",
            "a8e43ccb7cad4bf68682c6931f58e609",
            "59828cfedbf34117a74a949704c5681b",
            "6eb9dbdd777d4efab186494af90a6d95",
            "13f514d4efd94d449a06906a6368829b",
            "9da2e04d4e1f4f5c90a71f848a7e1052",
            "e1f6f23dede74df09b0198e02f1f6db6",
            "ea4ab4de7d0442d7b4d19db518b8eea2",
            "d6b9c955b6544c90ba70b5464c1673f5",
            "38af4c9e9e5c47c198dfe497caeb7600",
            "2f0708ca65864933af1b03368976bff6",
            "cedc1e4cd32d4f3e843df7332b16755f",
            "f67019af0e6e4cfca971e42658452c02",
            "ea1dea73c63b48358ea287baff39302e",
            "ba3823c221614412b1a7ed810a6b18e8",
            "70ea42f755b34ed2b02f3948a68d9278",
            "20e1baeaf2264de6abc78da812d156cd",
            "d5db8373002c42b1abfdbb17d9760e70",
            "8c4992635b314c5ab45f89dce3de178c",
            "6cc47575232c4dbcb652fade0b1e4c8f",
            "eaf564a52913445091b3364771cce9dc",
            "468c4de40611401092c889ec632f1f6f",
            "030e593ff7bd4d889211115fdd8a0d48",
            "28a4bd7d095347d583918dba6bc771cb",
            "12a613c0fc6246e4817b6e11b60fc584",
            "b6324a843c2c47a09133a526554c3cc5",
            "a6b5327eb3e343f2802e26d2210c76a0",
            "423ee62632f843fa9624c23bae77c45b",
            "6f722fa49bec4a83a0a0f3c80217d7b5",
            "5ed1a2b405b44bfc87951b868d3d2e43",
            "b25deda339454df6b9ccaaab13f9a878",
            "6e72585617624e3bb25e48cd44780715",
            "882d7dcf6d944abbb6a78eedc66890f6",
            "379421534c1a4a86a34da8fcaa984466",
            "56ad2c988b4546ed85eaedea76b08917",
            "d9f15c6837f14b77ab6c7074adece3f6",
            "724289cd30794cd081591f08067ace16",
            "89f94106224d45369aea05fc750b809b",
            "987231e88f664dfda70fdb9d33512780",
            "08e8e28c3e7546f38b7f3ac02cfba296",
            "f86cb965b2dc4a889d466a91e923b3fd",
            "d9ccc1dfe92e4550855ba4db936eed57",
            "00af3421231b419abf74cb7622978903",
            "5e626227a926452189900af35e0c7c6b",
            "04b5dc67c18b4ba28de3f6903554b21b",
            "5c94a1f8c95c4fb4b300ca4799fa9626",
            "ffed2f15fe6c41908d3c54fcf2a3b630",
            "bf7b51288f394af0aeb869f3bcfd3235",
            "6b970f7678284f1281f4cca1a4841c7d",
            "99d90feb73a64e2d86b1b1de24f0e116",
            "8b31560740a54b2db95c1e9e601c0083",
            "adad5d2924d840cabba2c644a3a07888",
            "cadd44a8167a48819335b2f31bb3177c",
            "f2c19d9e35a74d728f1635928c8bd2fc",
            "717c3de2377c4b63b11e1827a5b0f2be",
            "dc9bd72df41544919699b38679048b27",
            "776410b928d547fb90efea8e9da42d66",
            "4e1936cd4b6a40d5b69492689b4b5d08",
            "254ec9ce89224fb0b192d336e3afbf16",
            "095c44e9d2a94bcaa7444762886ea4a0",
            "a6de7efb5a1f440dbd9a7947cf9a6b84",
            "a95e1d43575a493b8f1fc90527daac04",
            "dae05e3ca784449a9a4fab9adf7f996a",
            "f8a7d9f75b674ca98ad39487ce579cc2",
            "74c8200271f34106a9af2664f6ad6201",
            "3ca1e21cd5c84cb3a79885aeb92f1747",
            "f1f61df48e004d319fc91cc819e8ab68",
            "050e781b1c2c4e23af6dac76a71c6f2b"
          ]
        },
        "outputId": "9f7b3610-f365-4921-df8e-5c1c7d1ddc7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Local log path created: MyDrive/thesis/hybrid_fusion/test_1/training_log.txt\n",
            "Google Drive log path created: /content/drive/MyDrive/thesis/hybrid_fusion/test_1/training_log.txt\n",
            "Logging started at 2025-03-07 06:56:25\n",
            "Log file: MyDrive/thesis/hybrid_fusion/test_1/training_log.txt\n",
            "Drive log file: /content/drive/MyDrive/thesis/hybrid_fusion/test_1/training_log.txt\n",
            "Loading datasets...\n",
            "\n",
            "******************************\n",
            "Dataset Summary:\n",
            "Training samples: 21497\n",
            "Validation samples: 2687\n",
            "Test samples: 2688\n",
            "Dataset summary:\n",
            "  - 21497 samples\n",
            "  - 11 categories\n",
            "  - 27 intents\n",
            "  - 19 NER tags\n",
            "Category distribution: {'cancel': 751, 'account': 4826, 'refund': 2391, 'contact': 1590, 'order': 3225, 'delivery': 1605, 'shipping': 1526, 'invoice': 1592, 'feedback': 1570, 'subscription': 833, 'payment': 1588}\n",
            "Intent distribution: {'check_cancellation_fee': 751, 'edit_account': 814, 'check_refund_policy': 797, 'contact_human_agent': 798, 'switch_account': 816, 'recover_password': 804, 'track_refund': 793, 'cancel_order': 811, 'delivery_options': 777, 'set_up_shipping_address': 769, 'change_shipping_address': 757, 'get_invoice': 784, 'review': 773, 'change_order': 810, 'contact_customer_service': 792, 'newsletter_subscription': 833, 'check_invoice': 808, 'payment_issue': 795, 'track_order': 797, 'check_payment_methods': 793, 'get_refund': 801, 'place_order': 807, 'create_account': 780, 'registration_problems': 795, 'complaint': 797, 'delete_account': 817, 'delivery_period': 828}\n",
            "NER tag distribution (non-O): {'B-account_category': 659, 'I-account_category': 659, 'B-refund_amount': 508, 'I-refund_amount': 508, 'B-order_number': 2352, 'I-order_number': 2352, 'B-delivery_country': 146, 'I-delivery_country': 146, 'B-person_name': 724, 'I-person_name': 724, 'B-account_type': 823, 'I-account_type': 823, 'B-currency_symbol': 309, 'I-currency_symbol': 309, 'B-delivery_city': 182, 'I-delivery_city': 182, 'B-invoice_number': 6, 'I-invoice_number': 6}\n",
            "Processing time: 0.066 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06f8e6320b3548d79f0f08235cec5b49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed105cef090a42d383db563ded9167c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1330ac25c24e48da8770f0b347a08e6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4e4bc5209104cd1a2992d8d36932136"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f53bd9554d340d0a19da3ad829677b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59828cfedbf34117a74a949704c5681b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea1dea73c63b48358ea287baff39302e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12a613c0fc6246e4817b6e11b60fc584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9f15c6837f14b77ab6c7074adece3f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffed2f15fe6c41908d3c54fcf2a3b630"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e1936cd4b6a40d5b69492689b4b5d08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model before training is on: cuda:0\n",
            "Training configuration saved to MyDrive/thesis/hybrid_fusion/test_1/training_config.json\n",
            "******************************\n",
            "Starting training...\n",
            "Model is on: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5:\n",
            "  Train Loss:      2.8445\n",
            "  Val Loss:       0.7934\n",
            "\n",
            "  Train Intent Acc: 0.4937\n",
            "  Val Intent Acc:  0.8217\n",
            "\n",
            "  Train Category F1:0.7067\n",
            "  Val Category F1: 0.9650\n",
            "\n",
            "  Train NER F1:     0.0644\n",
            "  Val NER F1:      0.4150\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5 [Training]:  25%|██▍       | 333/1344 [01:02<03:07,  5.40it/s, loss=0.756]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Artifacts(models, labels, tokenizer, etc)"
      ],
      "metadata": {
        "id": "XrjIlbTu9arS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save artifacts\n",
        "save_training_config(training_config, save_path)\n",
        "save_training_config_to_drive(training_config, save_path)\n",
        "save_artifacts(label_encoders, metrics, test_results, save_path)\n",
        "save_artifacts_to_drive(label_encoders, metrics, test_results, save_path)\n",
        "save_full_model(model, gpt2_tokenizer, distilbert_tokenizer, save_path)\n",
        "save_full_model_to_drive(model, gpt2_tokenizer, distilbert_tokenizer, save_path)"
      ],
      "metadata": {
        "id": "08otnCw79aPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vtk0XcBM9aHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "HC49miQf0RrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def run_inference(model_path: str, gpt2_tokenizer_path: str, distilbert_tokenizer_path: str, label_encoders_path: str, input_text: str, max_length: int = 128):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the full model\n",
        "    model = torch.load(model_path, map_location=device, weights_only=False)\n",
        "    model.eval()\n",
        "\n",
        "    # Load tokenizers\n",
        "    gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(gpt2_tokenizer_path)\n",
        "    distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained(distilbert_tokenizer_path)\n",
        "\n",
        "    # Load label encoders\n",
        "    with open(label_encoders_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        label_encoders = json.load(f)\n",
        "    intent_decoder = {v: k for k, v in label_encoders[\"intent_encoder\"].items()}\n",
        "    category_decoder = {v: k for k, v in label_encoders[\"category_encoder\"].items()}\n",
        "    ner_decoder = {v: k for k, v in label_encoders[\"ner_label_encoder\"].items()}\n",
        "\n",
        "    # Preprocess input\n",
        "    gpt2_inputs = gpt2_tokenizer(\n",
        "        input_text, return_tensors=\"pt\", max_length=max_length, padding=\"max_length\", truncation=True, return_offsets_mapping=True\n",
        "    )\n",
        "    distilbert_inputs = distilbert_tokenizer(\n",
        "        input_text, return_tensors=\"pt\", max_length=max_length, padding=\"max_length\", truncation=True\n",
        "    )\n",
        "    inputs = {\n",
        "        \"gpt2_input_ids\": gpt2_inputs[\"input_ids\"].to(device),\n",
        "        \"gpt2_attention_mask\": gpt2_inputs[\"attention_mask\"].to(device),\n",
        "        \"distilbert_input_ids\": distilbert_inputs[\"input_ids\"].to(device),\n",
        "        \"distilbert_attention_mask\": distilbert_inputs[\"attention_mask\"].to(device)\n",
        "    }\n",
        "    offset_mapping = gpt2_inputs[\"offset_mapping\"][0].cpu().tolist()\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        intent_logits = outputs[\"intent_logits\"]\n",
        "        intent_probs = F.softmax(intent_logits, dim=-1)[0]\n",
        "        intent_pred_idx = torch.argmax(intent_probs).item()\n",
        "        intent_confidence = intent_probs[intent_pred_idx].item()\n",
        "        intent_label = intent_decoder[intent_pred_idx]\n",
        "\n",
        "        category_logits = outputs[\"category_logits\"]\n",
        "        category_probs = F.softmax(category_logits, dim=-1)[0]\n",
        "        category_pred_idx = torch.argmax(category_probs).item()\n",
        "        category_confidence = category_probs[category_pred_idx].item()\n",
        "        category_label = category_decoder[category_pred_idx]\n",
        "\n",
        "        ner_logits = outputs[\"ner_logits\"][0]\n",
        "        ner_probs = F.softmax(ner_logits, dim=-1)\n",
        "        ner_pred_idxs = torch.argmax(ner_probs, dim=-1).tolist()\n",
        "        ner_confidences = torch.max(ner_probs, dim=-1).values.tolist()\n",
        "        ner_labels = [ner_decoder[idx] for idx in ner_pred_idxs]\n",
        "\n",
        "        seq_len = inputs[\"gpt2_attention_mask\"][0].sum().item()\n",
        "        ner_labels = ner_labels[:seq_len]\n",
        "        ner_confidences = ner_confidences[:seq_len]\n",
        "        offset_mapping = offset_mapping[:seq_len]\n",
        "\n",
        "    # Detect entity spans\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    entity_start = None\n",
        "    entity_confidences = []\n",
        "\n",
        "    for i, (label, conf, (start, end)) in enumerate(zip(ner_labels, ner_confidences, offset_mapping)):\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity is not None:\n",
        "                entity_text = input_text[entity_start:start]\n",
        "                entities.append({\n",
        "                    \"entity\": entity_text.strip(),\n",
        "                    \"label\": current_entity,\n",
        "                    \"confidence\": sum(entity_confidences) / len(entity_confidences)\n",
        "                })\n",
        "            current_entity = label[2:]\n",
        "            entity_start = start\n",
        "            entity_confidences = [conf]\n",
        "\n",
        "        elif label.startswith(\"I-\") and current_entity == label[2:]:\n",
        "            entity_confidences.append(conf)\n",
        "\n",
        "        elif label == \"O\" and current_entity is not None:\n",
        "            entity_text = input_text[entity_start:start]\n",
        "            entities.append({\n",
        "                \"entity\": entity_text.strip(),\n",
        "                \"label\": current_entity,\n",
        "                \"confidence\": sum(entity_confidences) / len(entity_confidences)\n",
        "            })\n",
        "            current_entity = None\n",
        "            entity_confidences = []\n",
        "\n",
        "    if current_entity is not None:\n",
        "        entity_text = input_text[entity_start:offset_mapping[-1][1]]\n",
        "        entities.append({\n",
        "            \"entity\": entity_text.strip(),\n",
        "            \"label\": current_entity,\n",
        "            \"confidence\": sum(entity_confidences) / len(entity_confidences)\n",
        "        })\n",
        "\n",
        "    results = {\n",
        "        \"intent\": {\"label\": intent_label, \"confidence\": intent_confidence},\n",
        "        \"category\": {\"label\": category_label, \"confidence\": category_confidence},\n",
        "        \"ner\": entities\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "input_text = \"I want a refund amount of $2323 now\"\n",
        "results = run_inference(\n",
        "    model_path=\"MyDrive/thesis/hybrid/test_1/model/full_model.pt\",\n",
        "    gpt2_tokenizer_path=\"MyDrive/thesis/hybrid/test_1/gpt2_tokenizer\",\n",
        "    distilbert_tokenizer_path=\"MyDrive/thesis/hybrid/test_1/distilbert_tokenizer\",\n",
        "    label_encoders_path=\"MyDrive/thesis/hybrid/test_1/label_encoders.json\",\n",
        "    input_text=input_text\n",
        ")\n",
        "\n",
        "print(\"Inference Results:\")\n",
        "print(f\"Intent: {results['intent']['label']} (Confidence: {results['intent']['confidence']:.4f})\")\n",
        "print(f\"Category: {results['category']['label']} (Confidence: {results['category']['confidence']:.4f})\")\n",
        "print(\"NER:\")\n",
        "for entity in results['ner']:\n",
        "    print(f\"  Entity: {entity['entity']} | Label: {entity['label']} | Confidence: {entity['confidence']:.4f}\")"
      ],
      "metadata": {
        "id": "ic73a58d1WY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}