{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reagan13/gpt2-distilbert-thesis-files/blob/main/notebook/Hybrid_Dense_Layer_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyDTXzpABDe4"
      },
      "source": [
        "# Hybrid V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAp09LrBFhJ"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJV3FtF8A3Nu",
        "outputId": "f40548ca-8b1c-40a1-e517-034b7e131f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import json\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Config, GPT2Model, DistilBertModel, GPT2TokenizerFast, DistilBertTokenizerFast\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "output_dir = \"/content/drive/MyDrive/thesis/Hybrid_Dense_Layer_Unfreeze\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "log_file = os.path.join(output_dir, \"training_log.txt\")\n",
        "\n",
        "def log_to_file(message: str):\n",
        "    with open(log_file, 'a') as f:\n",
        "        f.write(f\"{message}\\n\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "log_to_file(f\"Using device: {device}\")\n",
        "\n",
        "def check_device(tensor_or_model, name: str):\n",
        "    if isinstance(tensor_or_model, torch.nn.Module):\n",
        "        device_name = next(tensor_or_model.parameters()).device\n",
        "    else:\n",
        "        device_name = tensor_or_model.device\n",
        "    log_to_file(f\"{name} is on: {device_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0IcV0crBIO9"
      },
      "source": [
        "## Data Loading and Label Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp-UGq6FBJMf"
      },
      "outputs": [],
      "source": [
        "def load_dataset(json_file: str) -> List[Dict]:\n",
        "    log_to_file(f\"Loading dataset from {json_file}...\")\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    log_to_file(f\"Loaded {len(data)} samples from {json_file}\")\n",
        "    return data\n",
        "\n",
        "def detect_labels(data: List[Dict]) -> Dict[str, Dict]:\n",
        "    log_to_file(\"Detecting labels...\")\n",
        "    if not data:\n",
        "        log_to_file(\"Warning: Empty dataset\")\n",
        "        return {\"category_encoder\": {}, \"intent_encoder\": {}, \"ner_label_encoder\": {\"O\": 0}}\n",
        "\n",
        "    unique_categories = set()\n",
        "    unique_intents = set()\n",
        "    unique_ner_labels = set([\"O\"])\n",
        "\n",
        "    for i, sample in enumerate(data):\n",
        "        unique_categories.add(sample[\"category\"])\n",
        "        unique_intents.add(sample[\"intent\"])\n",
        "        for label in sample[\"ner_labels_only\"]:\n",
        "            label_type = label[\"label\"]\n",
        "            unique_ner_labels.add(f\"B-{label_type}\")\n",
        "            unique_ner_labels.add(f\"I-{label_type}\")\n",
        "\n",
        "    category_encoder = {cat: idx for idx, cat in enumerate(sorted(unique_categories))}\n",
        "    intent_encoder = {intent: idx for idx, intent in enumerate(sorted(unique_intents))}\n",
        "    ner_label_encoder = {ner: idx for idx, ner in enumerate(sorted(unique_ner_labels))}\n",
        "\n",
        "    log_to_file(f\"Label detection summary: Categories={len(category_encoder)}, Intents={len(intent_encoder)}, NER tags={len(ner_label_encoder)}\")\n",
        "    return {\"category_encoder\": category_encoder, \"intent_encoder\": intent_encoder, \"ner_label_encoder\": ner_label_encoder}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O92l0CFhDGWl"
      },
      "source": [
        "## Tokenization and Align Ner Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK-h2IZeDJgb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 3: Tokenization and NER Alignment\n",
        "def tokenize_text(text: str, gpt2_tokenizer, distilbert_tokenizer, max_length: int) -> Dict[str, torch.Tensor]:\n",
        "    gpt2_inputs = gpt2_tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    distilbert_inputs = distilbert_tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"gpt2_input_ids\": gpt2_inputs[\"input_ids\"].squeeze(0),\n",
        "        \"gpt2_attention_mask\": gpt2_inputs[\"attention_mask\"].squeeze(0),\n",
        "        \"distilbert_input_ids\": distilbert_inputs[\"input_ids\"].squeeze(0),\n",
        "        \"distilbert_attention_mask\": distilbert_inputs[\"attention_mask\"].squeeze(0)\n",
        "    }\n",
        "\n",
        "def align_ner_labels(text: str, ner_labels: List[Dict], tokenizer, ner_label_encoder: Dict, max_length: int, sample_idx: int = -1) -> torch.Tensor:\n",
        "    global print_counter\n",
        "    if 'print_counter' not in globals():\n",
        "        print_counter = 0\n",
        "\n",
        "    should_log = print_counter < 2 and sample_idx >= 0\n",
        "    if should_log:\n",
        "        log_to_file(f\"Aligning NER labels for text: {text[:50]}...\")\n",
        "        log_to_file(f\"NER Labels: {ner_labels}\")\n",
        "    print_counter += 1 if sample_idx >= 0 else 0\n",
        "\n",
        "    sorted_labels = sorted(ner_labels, key=lambda x: len(x[\"text\"]), reverse=True) if ner_labels else []\n",
        "    encoding = tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_offsets_mapping=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    token_to_char_map = encoding[\"offset_mapping\"][0].tolist()\n",
        "    ner_aligned = [ner_label_encoder[\"O\"]] * max_length\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    found_count = 0\n",
        "    not_found_count = 0\n",
        "\n",
        "    for label in sorted_labels:\n",
        "        if \"text\" not in label or \"label\" not in label:\n",
        "            if should_log:\n",
        "                log_to_file(f\"Warning: Skipping invalid NER entry {label}\")\n",
        "            continue\n",
        "        label_text, label_type = label[\"text\"], label[\"label\"]\n",
        "        label_text_lower = label_text.lower()\n",
        "        start_pos = 0\n",
        "        found_at_least_once = False\n",
        "        while True:\n",
        "            label_start = text_lower.find(label_text_lower, start_pos)\n",
        "            if label_start == -1:\n",
        "                if not found_at_least_once:\n",
        "                    not_found_count += 1\n",
        "                break\n",
        "            label_end = label_start + len(label_text_lower)\n",
        "            found_at_least_once = True\n",
        "            found_count += 1\n",
        "            start_pos = label_end\n",
        "            first_token = True\n",
        "            tokens_tagged = False\n",
        "            for i, (start, end) in enumerate(token_to_char_map):\n",
        "                if start == 0 and end == 0:\n",
        "                    continue\n",
        "                if start < label_end and end > label_start and end > start:\n",
        "                    prefix = \"B-\" if first_token else \"I-\"\n",
        "                    first_token = False\n",
        "                    ner_aligned[i] = ner_label_encoder.get(f\"{prefix}{label_type}\", ner_label_encoder[\"O\"])\n",
        "                    tokens_tagged = True\n",
        "            if not tokens_tagged and should_log:\n",
        "                log_to_file(f\"Warning: No tokens aligned for '{label_text}' ({label_type}) at {label_start}-{label_end}\")\n",
        "\n",
        "    if should_log:\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
        "        seq_len = encoding[\"attention_mask\"][0].sum().item()\n",
        "        log_to_file(\"Debug Aligned Labels:\")\n",
        "        log_to_file(\"Token | Aligned Label\")\n",
        "        for token, label_idx in zip(tokens[:seq_len], ner_aligned[:seq_len]):\n",
        "            label = list(ner_label_encoder.keys())[list(ner_label_encoder.values()).index(label_idx)]\n",
        "            log_to_file(f\"{token:<15} | {label}\")\n",
        "        log_to_file(f\"Alignment Summary: {found_count} entities found, {not_found_count} entities not found\")\n",
        "\n",
        "    return torch.tensor(ner_aligned, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pR_7azmBR8L"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djuZUYVRBSdd"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Dataset and DataLoader\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data: List[Dict], gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length: int):\n",
        "        self.data = data\n",
        "        self.gpt2_tokenizer = gpt2_tokenizer\n",
        "        self.distilbert_tokenizer = distilbert_tokenizer\n",
        "        self.label_encoders = label_encoders\n",
        "        self.max_length = max_length\n",
        "        log_to_file(f\"Initialized dataset with {len(data)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        text = sample[\"instruction\"]\n",
        "        inputs = tokenize_text(text, self.gpt2_tokenizer, self.distilbert_tokenizer, self.max_length)\n",
        "        ner_labels = align_ner_labels(text, sample[\"ner_labels_only\"], self.gpt2_tokenizer,\n",
        "                                      self.label_encoders[\"ner_label_encoder\"], self.max_length, sample_idx=idx)\n",
        "        return {\n",
        "            \"gpt2_input_ids\": inputs[\"gpt2_input_ids\"],\n",
        "            \"gpt2_attention_mask\": inputs[\"gpt2_attention_mask\"],\n",
        "            \"distilbert_input_ids\": inputs[\"distilbert_input_ids\"],\n",
        "            \"distilbert_attention_mask\": inputs[\"distilbert_attention_mask\"],\n",
        "            \"category_labels\": torch.tensor(self.label_encoders[\"category_encoder\"][sample[\"category\"]], dtype=torch.long),\n",
        "            \"intent_labels\": torch.tensor(self.label_encoders[\"intent_encoder\"][sample[\"intent\"]], dtype=torch.long),\n",
        "            \"ner_labels\": ner_labels\n",
        "        }\n",
        "\n",
        "def get_dataloaders(train_data, val_data, test_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, batch_size, num_workers, max_length):\n",
        "    pin_memory = device.type == \"cuda\"\n",
        "    log_to_file(\"Creating DataLoaders...\")\n",
        "    train_dataset = MultiTaskDataset(train_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length)\n",
        "    val_dataset = MultiTaskDataset(val_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length)\n",
        "    test_dataset = MultiTaskDataset(test_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    log_to_file(f\"DataLoaders created: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}\")\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rclenCRqBX7N"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngXyt4wjBZrr"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Model Definition\n",
        "class DenseFusionLayer(nn.Module):\n",
        "    def __init__(self, gpt2_dim: int, bert_dim: int, output_dim: int, dropout_rate: float):\n",
        "        super().__init__()\n",
        "        self.gpt2_proj = nn.Linear(gpt2_dim, output_dim)\n",
        "        self.bert_proj = nn.Linear(bert_dim, output_dim)\n",
        "        self.dense = nn.Linear(output_dim, output_dim)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, gpt2_features: torch.Tensor, bert_features: torch.Tensor,\n",
        "                attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        gpt2_proj = self.gpt2_proj(gpt2_features)\n",
        "        bert_proj = self.bert_proj(bert_features)\n",
        "        combined_features = gpt2_proj + bert_proj\n",
        "        fused_features = self.dense(combined_features)\n",
        "        fused_features = self.activation(fused_features)\n",
        "        fused_features = self.dropout(fused_features)\n",
        "        return self.layer_norm(fused_features)\n",
        "\n",
        "class HybridGPT2DistilBERTMultiTask(nn.Module):\n",
        "    def __init__(self, num_intents: int, num_categories: int, num_ner_labels: int,\n",
        "                 dropout_rate: float , loss_weights: Dict[str, float] ,\n",
        "                ner_class_weights: torch.Tensor, category_class_weights: torch.Tensor ,\n",
        "                 intent_class_weights: torch.Tensor ):\n",
        "        super().__init__()\n",
        "        log_to_file(\"Initializing model...\")\n",
        "        self.gpt2_config = GPT2Config.from_pretrained('gpt2')\n",
        "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "       # Option 1: Freeze all layers (default)\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last layer in GPT-2\n",
        "        # for name, param in self.gpt2.named_parameters():\n",
        "        #     if name.startswith('transformer.h.' + str(self.gpt2.config.n_layer - 1)):  # Check if it's the last layer\n",
        "        #         param.requires_grad = True\n",
        "        # log_to_file(\"Unfroze the last layer of GPT-2\")\n",
        "\n",
        "        for param in self.distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "        log_to_file(\" DistilBERT layers remain frozen by default\")\n",
        "\n",
        "\n",
        "        gpt2_dim = self.gpt2_config.n_embd\n",
        "        bert_dim = self.distilbert.config.hidden_size\n",
        "        hidden_size = gpt2_dim\n",
        "\n",
        "        self.fusion_layer = DenseFusionLayer(gpt2_dim, bert_dim, hidden_size, dropout_rate)\n",
        "\n",
        "        self.intent_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_intents)\n",
        "        )\n",
        "        self.category_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_categories)\n",
        "        )\n",
        "        self.ner_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_ner_labels)\n",
        "        )\n",
        "\n",
        "        self.intent_loss_fn = nn.CrossEntropyLoss(weight=intent_class_weights)\n",
        "        self.category_loss_fn = nn.CrossEntropyLoss(weight=category_class_weights)\n",
        "        self.ner_loss_fn = nn.CrossEntropyLoss(weight=ner_class_weights)\n",
        "        self.loss_weights = loss_weights or {'intent': 0.3, 'category': 0.3, 'ner': 0.4}\n",
        "        log_to_file(f\"Model initialized with loss weights: {self.loss_weights}\")\n",
        "        if intent_class_weights is not None:\n",
        "            log_to_file(f\"Intent class weights applied: {intent_class_weights[:5]}...\")\n",
        "        if category_class_weights is not None:\n",
        "            log_to_file(f\"Category class weights applied: {category_class_weights[:5]}...\")\n",
        "        if ner_class_weights is not None:\n",
        "            log_to_file(f\"NER class weights applied: {ner_class_weights[:5]}...\")\n",
        "\n",
        "    def forward(self, gpt2_input_ids: torch.Tensor, gpt2_attention_mask: torch.Tensor,\n",
        "                distilbert_input_ids: torch.Tensor, distilbert_attention_mask: torch.Tensor,\n",
        "                intent_labels: Optional[torch.Tensor] = None,\n",
        "                category_labels: Optional[torch.Tensor] = None,\n",
        "                ner_labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        gpt2_outputs = self.gpt2(input_ids=gpt2_input_ids, attention_mask=gpt2_attention_mask)\n",
        "        distilbert_outputs = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distilbert_attention_mask)\n",
        "\n",
        "        gpt2_features = gpt2_outputs.last_hidden_state\n",
        "        bert_features = distilbert_outputs.last_hidden_state\n",
        "\n",
        "        fused_features = self.fusion_layer(gpt2_features, bert_features, gpt2_attention_mask)\n",
        "\n",
        "        masked_features = fused_features * gpt2_attention_mask.unsqueeze(-1)\n",
        "        sequence_repr = masked_features.sum(dim=1) / gpt2_attention_mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "        intent_logits = self.intent_head(sequence_repr)\n",
        "        category_logits = self.category_head(sequence_repr)\n",
        "        ner_logits = self.ner_head(fused_features)\n",
        "\n",
        "        output_dict = {\n",
        "            'intent_logits': intent_logits,\n",
        "            'category_logits': category_logits,\n",
        "            'ner_logits': ner_logits\n",
        "        }\n",
        "\n",
        "        if all(label is not None for label in [intent_labels, category_labels, ner_labels]):\n",
        "            intent_loss = self.intent_loss_fn(intent_logits, intent_labels)\n",
        "            category_loss = self.category_loss_fn(category_logits, category_labels)\n",
        "            combined_mask = (gpt2_attention_mask * distilbert_attention_mask)\n",
        "            active_loss = combined_mask.view(-1) == 1\n",
        "            active_logits = ner_logits.view(-1, ner_logits.size(-1))[active_loss]\n",
        "            active_labels = ner_labels.view(-1)[active_loss]\n",
        "            ner_loss = self.ner_loss_fn(active_logits, active_labels)\n",
        "\n",
        "            total_loss = (self.loss_weights['intent'] * intent_loss +\n",
        "                          self.loss_weights['category'] * category_loss +\n",
        "                          self.loss_weights['ner'] * ner_loss)\n",
        "\n",
        "            output_dict.update({\n",
        "                'loss': total_loss,\n",
        "                'intent_loss': intent_loss,\n",
        "                'category_loss': category_loss,\n",
        "                'ner_loss': ner_loss\n",
        "            })\n",
        "\n",
        "        return output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZeS-qmMBb4e"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSZvneiPBdS5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 6: Training Function\n",
        "def train_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "    model.to(device)\n",
        "    check_device(model, \"Model\")\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_intent_f1\": [], \"val_intent_f1\": [],\n",
        "               \"train_category_f1\": [], \"val_category_f1\": [], \"train_ner_f1\": [], \"val_ner_f1\": []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        all_train_intent_preds, all_train_intent_labels = [], []\n",
        "        all_train_category_preds, all_train_category_labels = [], []\n",
        "        all_train_ner_preds, all_train_ner_labels = [], []\n",
        "\n",
        "        log_to_file(f\"Starting Epoch {epoch+1}/{num_epochs} [Training]\")\n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "            optimizer.zero_grad()\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            if i == 0:\n",
        "                check_device(inputs[\"gpt2_input_ids\"], \"GPT-2 Input IDs\")\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[\"loss\"]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            intent_preds = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().numpy()\n",
        "            category_preds = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().numpy()\n",
        "            ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()\n",
        "            all_train_intent_preds.extend(intent_preds)\n",
        "            all_train_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n",
        "            all_train_category_preds.extend(category_preds)\n",
        "            all_train_category_labels.extend(batch[\"category_labels\"].cpu().numpy())\n",
        "            all_train_ner_preds.extend(ner_preds.flatten())\n",
        "            all_train_ner_labels.extend(batch[\"ner_labels\"].cpu().numpy().flatten())\n",
        "\n",
        "        train_intent_f1 = precision_recall_fscore_support(all_train_intent_labels, all_train_intent_preds, average=\"macro\", zero_division=0)[2]\n",
        "        train_category_f1 = precision_recall_fscore_support(all_train_category_labels, all_train_category_preds, average=\"macro\", zero_division=0)[2]\n",
        "        train_ner_f1 = precision_recall_fscore_support(all_train_ner_labels, all_train_ner_preds, average=\"macro\", zero_division=0)[2]\n",
        "        history[\"train_loss\"].append(total_loss / len(train_loader))\n",
        "        history[\"train_intent_f1\"].append(float(train_intent_f1))  # Convert to float for JSON\n",
        "        history[\"train_category_f1\"].append(float(train_category_f1))\n",
        "        history[\"train_ner_f1\"].append(float(train_ner_f1))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_intent_preds, all_val_intent_labels = [], []\n",
        "        all_val_category_preds, all_val_category_labels = [], []\n",
        "        all_val_ner_preds, all_val_ner_labels = [], []\n",
        "\n",
        "        log_to_file(f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "                inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**inputs)\n",
        "                val_loss += outputs[\"loss\"].item()\n",
        "                intent_preds = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().numpy()\n",
        "                category_preds = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().numpy()\n",
        "                ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()\n",
        "                all_val_intent_preds.extend(intent_preds)\n",
        "                all_val_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n",
        "                all_val_category_preds.extend(category_preds)\n",
        "                all_val_category_labels.extend(batch[\"category_labels\"].cpu().numpy())\n",
        "                all_val_ner_preds.extend(ner_preds.flatten())\n",
        "                all_val_ner_labels.extend(batch[\"ner_labels\"].cpu().numpy().flatten())\n",
        "\n",
        "        val_intent_f1 = precision_recall_fscore_support(all_val_intent_labels, all_val_intent_preds, average=\"macro\", zero_division=0)[2]\n",
        "        val_category_f1 = precision_recall_fscore_support(all_val_category_labels, all_val_category_preds, average=\"macro\", zero_division=0)[2]\n",
        "        val_ner_f1 = precision_recall_fscore_support(all_val_ner_labels, all_val_ner_preds, average=\"macro\", zero_division=0)[2]\n",
        "        history[\"val_loss\"].append(val_loss / len(val_loader))\n",
        "        history[\"val_intent_f1\"].append(float(val_intent_f1))\n",
        "        history[\"val_category_f1\"].append(float(val_category_f1))\n",
        "        history[\"val_ner_f1\"].append(float(val_ner_f1))\n",
        "\n",
        "        log_to_file(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        log_to_file(f\"  Train Loss: {history['train_loss'][-1]:.4f}, Intent F1: {train_intent_f1:.4f}, Category F1: {train_category_f1:.4f}, NER F1: {train_ner_f1:.4f}\")\n",
        "        log_to_file(f\"  Val Loss: {history['val_loss'][-1]:.4f}, Intent F1: {val_intent_f1:.4f}, Category F1: {val_category_f1:.4f}, NER F1: {val_ner_f1:.4f}\")\n",
        "    return history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uyt0JvkBfzZ"
      },
      "source": [
        "## Eval Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjJ-BW_ZBhN1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 7: Evaluation Function\n",
        "def evaluate_model(model, test_loader, label_encoders, gpt2_tokenizer):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_intent_preds, all_intent_labels = [], []\n",
        "    all_category_preds, all_category_labels = [], []\n",
        "    all_ner_preds, all_ner_labels = [], []\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    log_to_file(\"Evaluating model on test set...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluation\")):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**inputs)\n",
        "            total_loss += outputs[\"loss\"].item()\n",
        "            intent_preds = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().numpy()\n",
        "            category_preds = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().numpy()\n",
        "            ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()\n",
        "            all_intent_preds.extend(intent_preds)\n",
        "            all_intent_labels.extend(batch[\"intent_labels\"].cpu().numpy())\n",
        "            all_category_preds.extend(category_preds)\n",
        "            all_category_labels.extend(batch[\"category_labels\"].cpu().numpy())\n",
        "            all_ner_preds.extend(ner_preds.flatten())\n",
        "            all_ner_labels.extend(batch[\"ner_labels\"].cpu().numpy().flatten())\n",
        "\n",
        "            if i == 0:\n",
        "                tokens = gpt2_tokenizer.convert_ids_to_tokens(inputs[\"gpt2_input_ids\"][0].tolist())\n",
        "                log_to_file(\"Sample Prediction (First Batch):\")\n",
        "                log_to_file(\"Token | Predicted NER | True NER\")\n",
        "                for token, pred, true in zip(tokens, ner_preds[0], batch[\"ner_labels\"][0].cpu().numpy()):\n",
        "                    pred_label = list(label_encoders[\"ner_label_encoder\"].keys())[list(label_encoders[\"ner_label_encoder\"].values()).index(pred)]\n",
        "                    true_label = list(label_encoders[\"ner_label_encoder\"].keys())[list(label_encoders[\"ner_label_encoder\"].values()).index(true)]\n",
        "                    log_to_file(f\"{token:<15} | {pred_label:<15} | {true_label}\")\n",
        "\n",
        "    intent_f1 = precision_recall_fscore_support(all_intent_labels, all_intent_preds, average=\"macro\", zero_division=0)[2]\n",
        "    category_f1 = precision_recall_fscore_support(all_category_labels, all_category_preds, average=\"macro\", zero_division=0)[2]\n",
        "    ner_f1 = precision_recall_fscore_support(all_ner_labels, all_ner_preds, average=\"macro\", zero_division=0)[2]\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    log_to_file(f\"Test Results:\")\n",
        "    log_to_file(f\"  Loss: {avg_loss:.4f}\")\n",
        "    log_to_file(f\"  Intent F1: {intent_f1:.4f}\")\n",
        "    log_to_file(f\"  Category F1: {category_f1:.4f}\")\n",
        "    log_to_file(f\"  NER F1: {ner_f1:.4f}\")\n",
        "    return {\"loss\": float(avg_loss), \"intent_f1\": float(intent_f1), \"category_f1\": float(category_f1), \"ner_f1\": float(ner_f1)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4SJhKwbBjJ-"
      },
      "source": [
        "## Main Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GPuc4TQBkOw"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3728fvUzBlfU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 8: Main Execution\n",
        "train_file = \"train.json\"\n",
        "val_file = \"val.json\"\n",
        "test_file = \"test.json\"\n",
        "batch_size = 16\n",
        "num_epochs = 3\n",
        "learning_rate = 2e-5\n",
        "max_length = 128\n",
        "num_workers = 2\n",
        "dropout_rate = 0.2\n",
        "loss_weights = {\"intent\": 0.2, \"category\": 0.2, \"ner\": 0.4}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7eBKhS6BoX0"
      },
      "source": [
        "### Load Data, Label encoders and Compute Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vxft0QYBo9M"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data = load_dataset(train_file)\n",
        "val_data = load_dataset(val_file)\n",
        "test_data = load_dataset(test_file)\n",
        "log_to_file(f\"Dataset sizes: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
        "if len(train_data) == 0:\n",
        "    raise ValueError(\"Training dataset is empty!\")\n",
        "\n",
        "label_encoders = detect_labels(train_data)\n",
        "\n",
        "# Compute class weights\n",
        "ner_counts = Counter()\n",
        "for sample in train_data:\n",
        "    for label in sample[\"ner_labels_only\"]:\n",
        "        ner_counts[f\"B-{label['label']}\"] += 1\n",
        "        ner_counts[f\"I-{label['label']}\"] += 1\n",
        "ner_counts[\"O\"] = sum(ner_counts.values()) * 10\n",
        "total_ner = sum(ner_counts.values())\n",
        "ner_class_weights = torch.tensor([total_ner / (len(label_encoders[\"ner_label_encoder\"]) * ner_counts.get(tag, 1))\n",
        "                                  for tag in label_encoders[\"ner_label_encoder\"]], dtype=torch.float).to(device)\n",
        "\n",
        "category_counts = Counter()\n",
        "for sample in train_data:\n",
        "    category_counts[sample[\"category\"]] += 1\n",
        "total_category = sum(category_counts.values())\n",
        "category_class_weights = torch.tensor([total_category / (len(label_encoders[\"category_encoder\"]) * category_counts.get(cat, 1))\n",
        "                                       for cat in label_encoders[\"category_encoder\"]], dtype=torch.float).to(device)\n",
        "\n",
        "intent_counts = Counter()\n",
        "for sample in train_data:\n",
        "    intent_counts[sample[\"intent\"]] += 1\n",
        "total_intent = sum(intent_counts.values())\n",
        "intent_class_weights = torch.tensor([total_intent / (len(label_encoders[\"intent_encoder\"]) * intent_counts.get(intent, 1))\n",
        "                                     for intent in label_encoders[\"intent_encoder\"]], dtype=torch.float).to(device)\n",
        "\n",
        "log_to_file(f\"NER class weights computed: {ner_class_weights[:5]}...\")\n",
        "log_to_file(f\"Category class weights computed: {category_class_weights[:5]}...\")\n",
        "log_to_file(f\"Intent class weights computed: {intent_class_weights[:5]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFjTkhsqBwg-"
      },
      "source": [
        "### Tokenizer and Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhJ6jMDnBzMA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "if gpt2_tokenizer.pad_token is None:\n",
        "    gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "if distilbert_tokenizer.pad_token is None:\n",
        "    distilbert_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "log_to_file(\"Tokenizers initialized\")\n",
        "\n",
        "train_loader, val_loader, test_loader = get_dataloaders(\n",
        "    train_data, val_data, test_data, gpt2_tokenizer, distilbert_tokenizer, label_encoders, batch_size, num_workers, max_length\n",
        ")\n",
        "\n",
        "model = HybridGPT2DistilBERTMultiTask(\n",
        "    num_intents=len(label_encoders[\"intent_encoder\"]),\n",
        "    num_categories=len(label_encoders[\"category_encoder\"]),\n",
        "    num_ner_labels=len(label_encoders[\"ner_label_encoder\"]),\n",
        "    dropout_rate=dropout_rate,\n",
        "    loss_weights=loss_weights,\n",
        "    ner_class_weights=ner_class_weights,\n",
        "    category_class_weights=category_class_weights,\n",
        "    intent_class_weights=intent_class_weights\n",
        ")\n",
        "if gpt2_tokenizer.pad_token_id is not None:\n",
        "    model.gpt2.resize_token_embeddings(len(gpt2_tokenizer))\n",
        "model.to(device)\n",
        "check_device(model, \"Model before training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8tQ3pVkEFvU"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaLC49SvEHzs",
        "outputId": "caa82f0e-16e0-4ebd-c254-e77fe66abf2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1344/1344 [10:02<00:00,  2.23it/s]\n",
            "Validation: 100%|██████████| 168/168 [00:31<00:00,  5.27it/s]\n",
            "Training: 100%|██████████| 1344/1344 [10:05<00:00,  2.22it/s]\n",
            "Validation: 100%|██████████| 168/168 [00:31<00:00,  5.26it/s]\n",
            "Training: 100%|██████████| 1344/1344 [10:06<00:00,  2.22it/s]\n",
            "Validation: 100%|██████████| 168/168 [00:32<00:00,  5.24it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save hyperparameters\n",
        "hyperparameters = {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"max_length\": max_length,\n",
        "    \"num_workers\": num_workers,\n",
        "    \"dropout_rate\": dropout_rate,\n",
        "    \"loss_weights\": loss_weights,\n",
        "    \"num_intents\": len(label_encoders[\"intent_encoder\"]),\n",
        "    \"num_categories\": len(label_encoders[\"category_encoder\"]),\n",
        "    \"num_ner_labels\": len(label_encoders[\"ner_label_encoder\"])\n",
        "}\n",
        "with open(os.path.join(output_dir, \"hyperparameters.json\"), 'w') as f:\n",
        "    json.dump(hyperparameters, f, indent=4)\n",
        "log_to_file(\"Hyperparameters saved to hyperparameters.json\")\n",
        "\n",
        "start_time = time.time()\n",
        "history = train_model(model, train_loader, val_loader, num_epochs, learning_rate)\n",
        "training_time = (time.time() - start_time) / 60\n",
        "log_to_file(f\"Training completed in {training_time:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lFaLR6EwTq"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMREhRGkELYr"
      },
      "source": [
        "#### Save Training History, Test Results and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9uDajo-lKRFi"
      },
      "outputs": [],
      "source": [
        "# During training, save:\n",
        "with open(os.path.join(output_dir, \"label_encoders.json\"), 'w') as f:\n",
        "    json.dump(label_encoders, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xZgLZhS3ERlW",
        "outputId": "033ae107-c721-4b42-ac85-4a3213ab2bf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 168/168 [00:32<00:00,  5.19it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save training history\n",
        "with open(os.path.join(output_dir, \"training_history.json\"), 'w') as f:\n",
        "    json.dump(history, f, indent=4)\n",
        "log_to_file(\"Training history saved to training_history.json\")\n",
        "\n",
        "test_results = evaluate_model(model, test_loader, label_encoders, gpt2_tokenizer)\n",
        "\n",
        "# Save test results\n",
        "with open(os.path.join(output_dir, \"test_results.json\"), 'w') as f:\n",
        "    json.dump(test_results, f, indent=4)\n",
        "log_to_file(\"Test results saved to test_results.json\")\n",
        "\n",
        "# Save model\n",
        "model_path = os.path.join(output_dir, \"hybrid_model.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "log_to_file(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aug1w98CHVoU"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTD7siJmHWlJ",
        "outputId": "85f9b184-9484-4e41-f295-262cf13e81df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded label encoders from label_encoders.json\n",
            "All GPT-2 and DistilBERT layers remain frozen by default\n",
            "Inference on text: where is ordered ord-5961074...\n",
            "Predicted Intent: track_order\n",
            "Predicted Category: order\n",
            "NER Predictions:\n",
            "Token | Predicted NER\n",
            "where           | O\n",
            "Ġis             | O\n",
            "Ġordered        | O\n",
            "Ġord            | B-order_number\n",
            "-               | I-order_number\n",
            "59              | I-order_number\n",
            "610             | I-order_number\n",
            "74              | I-order_number\n",
            "Inference results: {'intent': 'track_order', 'category': 'order', 'ner': [('where', 'O'), ('Ġis', 'O'), ('Ġordered', 'O'), ('Ġord', 'B-order_number'), ('-', 'I-order_number'), ('59', 'I-order_number'), ('610', 'I-order_number'), ('74', 'I-order_number')]}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2TokenizerFast, DistilBertTokenizerFast, GPT2Config, GPT2Model, DistilBertModel\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the DenseFusionLayer (for dense fusion hybrid)\n",
        "class DenseFusionLayer(nn.Module):\n",
        "    def __init__(self, gpt2_dim: int, bert_dim: int, output_dim: int, dropout_rate: float):\n",
        "        super().__init__()\n",
        "        self.gpt2_proj = nn.Linear(gpt2_dim, output_dim)\n",
        "        self.bert_proj = nn.Linear(bert_dim, output_dim)\n",
        "        self.dense = nn.Linear(output_dim, output_dim)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.layer_norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, gpt2_features: torch.Tensor, bert_features: torch.Tensor,\n",
        "                attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        gpt2_proj = self.gpt2_proj(gpt2_features)\n",
        "        bert_proj = self.bert_proj(bert_features)\n",
        "        combined_features = gpt2_proj + bert_proj  # Element-wise addition\n",
        "        fused_features = self.dense(combined_features)\n",
        "        fused_features = self.activation(fused_features)\n",
        "        fused_features = self.dropout(fused_features)\n",
        "        return self.layer_norm(fused_features)\n",
        "\n",
        "# Define the updated hybrid model with dense fusion\n",
        "class HybridGPT2DistilBERTMultiTask(nn.Module):\n",
        "    def __init__(self, num_intents: int, num_categories: int, num_ner_labels: int,\n",
        "                 dropout_rate: float,\n",
        "                 loss_weights: dict = None,  # Optional for inference\n",
        "                 ner_class_weights: torch.Tensor = None,  # Optional\n",
        "                 category_class_weights: torch.Tensor = None,  # Optional\n",
        "                 intent_class_weights: torch.Tensor = None):  # Optional\n",
        "        super().__init__()\n",
        "        self.gpt2_config = GPT2Config.from_pretrained('gpt2')\n",
        "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # Freeze all layers (default, matching your latest setup)\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"All GPT-2 and DistilBERT layers remain frozen by default\")\n",
        "\n",
        "        # Option to unfreeze last 2 layers (commented out, as per your latest test)\n",
        "        \"\"\"\n",
        "        # Unfreeze last 2 layers of GPT-2\n",
        "        for param in self.gpt2.h[-2:].parameters():\n",
        "            param.requires_grad = True\n",
        "        # Unfreeze last 2 layers of DistilBERT\n",
        "        for param in self.distilbert.transformer.layer[-2:].parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"Unfroze last 2 layers of GPT-2 and DistilBERT\")\n",
        "        \"\"\"\n",
        "\n",
        "        gpt2_dim = self.gpt2_config.n_embd  # 768\n",
        "        bert_dim = self.distilbert.config.hidden_size  # 768\n",
        "        hidden_size = gpt2_dim\n",
        "\n",
        "        # Use DenseFusionLayer instead of FusionLayer\n",
        "        self.fusion_layer = DenseFusionLayer(gpt2_dim, bert_dim, hidden_size, dropout_rate)\n",
        "\n",
        "        self.intent_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_intents)\n",
        "        )\n",
        "        self.category_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_categories)\n",
        "        )\n",
        "        self.ner_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, num_ner_labels)\n",
        "        )\n",
        "\n",
        "        # Define loss functions (optional for inference)\n",
        "        self.intent_loss_fn = nn.CrossEntropyLoss(weight=intent_class_weights) if intent_class_weights is not None else nn.CrossEntropyLoss()\n",
        "        self.category_loss_fn = nn.CrossEntropyLoss(weight=category_class_weights) if category_class_weights is not None else nn.CrossEntropyLoss()\n",
        "        self.ner_loss_fn = nn.CrossEntropyLoss(weight=ner_class_weights) if ner_class_weights is not None else nn.CrossEntropyLoss()\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def forward(self, gpt2_input_ids: torch.Tensor, gpt2_attention_mask: torch.Tensor,\n",
        "                distilbert_input_ids: torch.Tensor, distilbert_attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        gpt2_outputs = self.gpt2(input_ids=gpt2_input_ids, attention_mask=gpt2_attention_mask)\n",
        "        distilbert_outputs = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distilbert_attention_mask)\n",
        "\n",
        "        gpt2_features = gpt2_outputs.last_hidden_state\n",
        "        bert_features = distilbert_outputs.last_hidden_state\n",
        "\n",
        "        fused_features = self.fusion_layer(gpt2_features, bert_features, gpt2_attention_mask)\n",
        "\n",
        "        masked_features = fused_features * gpt2_attention_mask.unsqueeze(-1)\n",
        "        sequence_repr = masked_features.sum(dim=1) / gpt2_attention_mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "        intent_logits = self.intent_head(sequence_repr)\n",
        "        category_logits = self.category_head(sequence_repr)\n",
        "        ner_logits = self.ner_head(fused_features)\n",
        "\n",
        "        return {'intent_logits': intent_logits, 'category_logits': category_logits, 'ner_logits': ner_logits}\n",
        "\n",
        "# Tokenization function (unchanged)\n",
        "def tokenize_text(text: str, gpt2_tokenizer, distilbert_tokenizer, max_length: int) -> Dict[str, torch.Tensor]:\n",
        "    gpt2_inputs = gpt2_tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    distilbert_inputs = distilbert_tokenizer(\n",
        "        text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"gpt2_input_ids\": gpt2_inputs[\"input_ids\"].squeeze(0),\n",
        "        \"gpt2_attention_mask\": gpt2_inputs[\"attention_mask\"].squeeze(0),\n",
        "        \"distilbert_input_ids\": distilbert_inputs[\"input_ids\"].squeeze(0),\n",
        "        \"distilbert_attention_mask\": distilbert_inputs[\"attention_mask\"].squeeze(0)\n",
        "    }\n",
        "\n",
        "# Inference function (unchanged)\n",
        "def inference(model, text: str, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length: int, device):\n",
        "    model.eval()\n",
        "    print(f\"Inference on text: {text[:50]}...\")\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenize_text(text, gpt2_tokenizer, distilbert_tokenizer, max_length)\n",
        "    inputs = {k: v.unsqueeze(0).to(device) for k, v in inputs.items()}  # Add batch dimension\n",
        "\n",
        "    # Run model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Decode predictions\n",
        "    intent_pred = torch.argmax(outputs[\"intent_logits\"], dim=-1).cpu().item()\n",
        "    category_pred = torch.argmax(outputs[\"category_logits\"], dim=-1).cpu().item()\n",
        "    ner_preds = torch.argmax(outputs[\"ner_logits\"], dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    # Map to labels\n",
        "    intent_label = list(label_encoders[\"intent_encoder\"].keys())[list(label_encoders[\"intent_encoder\"].values()).index(intent_pred)]\n",
        "    category_label = list(label_encoders[\"category_encoder\"].keys())[list(label_encoders[\"category_encoder\"].values()).index(category_pred)]\n",
        "    tokens = gpt2_tokenizer.convert_ids_to_tokens(inputs[\"gpt2_input_ids\"][0].tolist())\n",
        "    seq_len = inputs[\"gpt2_attention_mask\"][0].sum().item()\n",
        "    ner_labels = [list(label_encoders[\"ner_label_encoder\"].keys())[list(label_encoders[\"ner_label_encoder\"].values()).index(pred)]\n",
        "                  for pred in ner_preds[:seq_len]]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Predicted Intent: {intent_label}\")\n",
        "    print(f\"Predicted Category: {category_label}\")\n",
        "    print(\"NER Predictions:\")\n",
        "    print(\"Token | Predicted NER\")\n",
        "    for token, ner_label in zip(tokens[:seq_len], ner_labels):\n",
        "        print(f\"{token:<15} | {ner_label}\")\n",
        "\n",
        "    return {\"intent\": intent_label, \"category\": category_label, \"ner\": list(zip(tokens[:seq_len], ner_labels))}\n",
        "\n",
        "# Load necessary components\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "output_dir = \"/content/drive/MyDrive/thesis/Hybrid_Dense_Layer_V2\"  # Update to dense fusion output dir\n",
        "model_path = os.path.join(output_dir, \"hybrid_model.pth\")\n",
        "\n",
        "# Load saved label encoders\n",
        "with open(os.path.join(output_dir, \"label_encoders.json\"), 'r') as f:\n",
        "    label_encoders = json.load(f)\n",
        "print(\"Loaded label encoders from label_encoders.json\")\n",
        "\n",
        "# Load tokenizers\n",
        "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "if gpt2_tokenizer.pad_token is None:\n",
        "    gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "if distilbert_tokenizer.pad_token is None:\n",
        "    distilbert_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load hyperparameters (for max_length and dropout_rate)\n",
        "with open(os.path.join(output_dir, \"hyperparameters.json\"), 'r') as f:\n",
        "    hyperparameters = json.load(f)\n",
        "\n",
        "# Initialize model with dense fusion parameters\n",
        "model = HybridGPT2DistilBERTMultiTask(\n",
        "    num_intents=len(label_encoders[\"intent_encoder\"]),\n",
        "    num_categories=len(label_encoders[\"category_encoder\"]),\n",
        "    num_ner_labels=len(label_encoders[\"ner_label_encoder\"]),\n",
        "    dropout_rate=hyperparameters[\"dropout_rate\"]\n",
        ")\n",
        "\n",
        "# Resize embeddings before loading state dict to match training\n",
        "if gpt2_tokenizer.pad_token_id is not None:\n",
        "    model.gpt2.resize_token_embeddings(len(gpt2_tokenizer))\n",
        "\n",
        "# Load state dict with strict=False to ignore loss function weights\n",
        "model.load_state_dict(torch.load(model_path, weights_only=True), strict=False)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Perform inference\n",
        "example_text = \"where is ordered ord-5961074\"\n",
        "results = inference(model, example_text, gpt2_tokenizer, distilbert_tokenizer, label_encoders, max_length=hyperparameters[\"max_length\"], device=device)\n",
        "print(f\"Inference results: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a5iW7vDJSnw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}