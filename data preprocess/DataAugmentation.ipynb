{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from nltk.corpus import wordnet\n",
    "from easynmt import EasyNMT\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models and Define NER Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "translator = EasyNMT('opus-mt')  # Local translation (fast)\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-small\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Define valid NER patterns (based on your original dataset)\n",
    "VALID_NER_PATTERNS = [\n",
    "    r\"order number\",\n",
    "    r\"invoice number\",\n",
    "    r\"person name\",\n",
    "    r\"account type\",\n",
    "    r\"account category\",\n",
    "    r\"delivery city\",\n",
    "    r\"delivery country\",\n",
    "    r\"currency symbol\",\n",
    "    r\"refund amount\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to preserve placeholders\n",
    "def preserve_placeholders(text, augmented_text):\n",
    "    \"\"\"\n",
    "    Replace placeholders in augmented_text with the original placeholders from text.\n",
    "    \"\"\"\n",
    "    placeholders = re.findall(r\"{{.*?}}\", text)  # Extract placeholders from the original text\n",
    "    for placeholder in placeholders:\n",
    "        augmented_text = re.sub(re.escape(placeholder), placeholder, augmented_text)  # Ensure placeholders remain unchanged\n",
    "    return augmented_text\n",
    "\n",
    "# Helper function to preserve NER entities\n",
    "def preserve_ner_entities(text):\n",
    "    \"\"\"\n",
    "    Identify and protect NER entities in the text by marking them.\n",
    "    \"\"\"\n",
    "    ner_entities = []\n",
    "    for pattern in VALID_NER_PATTERNS:\n",
    "        matches = re.findall(pattern, text)\n",
    "        for match in matches:\n",
    "            ner_entities.append(match)\n",
    "            text = text.replace(match, f\"[[{match}]]\")  # Mark NER entities to protect them\n",
    "    return text, ner_entities\n",
    "\n",
    "# Helper function to restore NER entities\n",
    "def restore_ner_entities(text, ner_entities):\n",
    "    \"\"\"\n",
    "    Restore NER entities after augmentation.\n",
    "    \"\"\"\n",
    "    for entity in ner_entities:\n",
    "        text = text.replace(f\"[[{entity}]]\", entity)  # Restore original NER entities\n",
    "    return text\n",
    "\n",
    "# Augmentation functions\n",
    "def synonym_replace(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if re.match(r\"{{.*?}}\", word) or re.match(r\"\\[\\[.*?\\]\\]\", word):  # Skip placeholders and NER entities\n",
    "            new_words.append(word)\n",
    "            continue\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            new_word = synonyms[0].lemmas()[0].name()\n",
    "            new_words.append(new_word if new_word != word else word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words), \"Synonym Replacement\"\n",
    "\n",
    "def add_noise(text):\n",
    "    words = text.split()\n",
    "    noise_type = random.choice([\"insertion\", \"deletion\"])  # Reduced noise types to avoid disarranging\n",
    "    if noise_type == \"insertion\":\n",
    "        words.insert(random.randint(0, len(words)), random.choice([\"uh\", \"well\", \"hmm\"]))\n",
    "    elif noise_type == \"deletion\" and len(words) > 1:\n",
    "        words.pop(random.randint(0, len(words) - 1))\n",
    "    return \" \".join(words), f\"Noise ({noise_type})\"\n",
    "\n",
    "def paraphrase(text):\n",
    "    return paraphraser(text, max_length=50, do_sample=True)[0][\"generated_text\"], \"Paraphrasing\"\n",
    "\n",
    "# Validation function to ensure NER patterns are preserved\n",
    "def validate_augmentation(original_text, augmented_text):\n",
    "    \"\"\"\n",
    "    Validate that the augmented text retains the original NER patterns.\n",
    "    \"\"\"\n",
    "    original_entities = set()\n",
    "    augmented_entities = set()\n",
    "    for pattern in VALID_NER_PATTERNS:\n",
    "        original_entities.update(re.findall(pattern, original_text))\n",
    "        augmented_entities.update(re.findall(pattern, augmented_text))\n",
    "    return original_entities == augmented_entities\n",
    "\n",
    "# Weighted augmentation selection\n",
    "def select_augmentation():\n",
    "    choices = [\n",
    "        (paraphrase, 40),\n",
    "        (synonym_replace, 30),\n",
    "        (random.choice([add_noise, paraphrase, synonym_replace]), 30)\n",
    "    ]\n",
    "    methods, weights = zip(*choices)\n",
    "    return random.choices(methods, weights=weights, k=1)[0]\n",
    "\n",
    "# Randomized augmentation function with progress tracking\n",
    "def augment_instruction(row):\n",
    "    instruction = row[\"instruction\"]\n",
    "\n",
    "    # Preserve placeholders and NER entities\n",
    "    instruction_with_ner, ner_entities = preserve_ner_entities(instruction)\n",
    "\n",
    "    chosen_method = select_augmentation()\n",
    "    try:\n",
    "        augmented_instruction, technique = chosen_method(instruction_with_ner)\n",
    "        augmented_instruction = restore_ner_entities(augmented_instruction, ner_entities)  # Restore NER entities\n",
    "        augmented_instruction = preserve_placeholders(instruction, augmented_instruction)  # Preserve placeholders\n",
    "\n",
    "        # Validate the augmented text\n",
    "        if not validate_augmentation(instruction, augmented_instruction):\n",
    "            raise ValueError(\"Augmented text does not retain original NER patterns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during augmentation: {e}\")  # Log the error message\n",
    "        augmented_instruction, technique = instruction, \"None (Fallback)\"\n",
    "\n",
    "    return {\n",
    "        \"category\": row[\"category\"],\n",
    "        \"intent\": row[\"intent\"],\n",
    "        \"response\": row[\"response\"],\n",
    "        \"instruction_original\": instruction,\n",
    "        \"instruction_augmented\": augmented_instruction,\n",
    "        \"augmentation_technique\": technique\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Augmentation and Save Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Augment only 30% of rows for each intent\n",
    "augmented_data = []\n",
    "\n",
    "# Process all intents\n",
    "for intent in tqdm(df[\"intent\"].unique(), desc=\"Processing Intents\"):\n",
    "    intent_df = df[df[\"intent\"] == intent]\n",
    "    num_rows = len(intent_df)\n",
    "    num_to_augment = int(0.3 * num_rows)  # 30% of rows for augmentation\n",
    "    rows_to_augment = intent_df.sample(n=num_to_augment, random_state=42)  # Randomly select rows\n",
    "    rows_to_keep = intent_df.drop(rows_to_augment.index)  # Remaining rows\n",
    "\n",
    "    # Augment selected rows\n",
    "    for _, row in tqdm(rows_to_augment.iterrows(), total=num_to_augment, desc=f\"Augmenting Intent: {intent}\"):\n",
    "        augmented_data.append(augment_instruction(row))\n",
    "\n",
    "    # Keep remaining rows unchanged\n",
    "    for _, row in rows_to_keep.iterrows():\n",
    "        augmented_data.append({\n",
    "            \"category\": row[\"category\"],\n",
    "            \"intent\": row[\"intent\"],\n",
    "            \"response\": row[\"response\"],\n",
    "            \"instruction_original\": row[\"instruction\"],\n",
    "            \"instruction_augmented\": row[\"instruction\"],\n",
    "            \"augmentation_technique\": \"None (Unchanged)\"\n",
    "        })\n",
    "\n",
    "# Convert back to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "augmented_df.to_csv(\"augmented_dataset.csv\", index=False)\n",
    "print(\"âœ… Data augmentation complete! Saved as 'augmented_dataset.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
